{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, ABSADataset\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, opt):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.lstm = DynamicLSTM(opt.embed_dim, opt.hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.dense = nn.Linear(opt.hidden_dim, opt.polarities_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_raw_indices = inputs[0]\n",
    "        x = self.embed(text_raw_indices)\n",
    "        x_len = torch.sum(text_raw_indices != 0, dim=-1)\n",
    "        _, (h_n, _) = self.lstm(x, x_len)\n",
    "        out = self.dense(h_n[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationEncoding(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(LocationEncoding, self).__init__()\n",
    "        self.opt = opt\n",
    "\n",
    "    def forward(self, x, pos_inx):\n",
    "        batch_size, seq_len = x.size()[0], x.size()[1]\n",
    "        weight = self.weight_matrix(pos_inx, batch_size, seq_len).to(self.opt.device)\n",
    "        x = weight.unsqueeze(2) * x\n",
    "        return x\n",
    "\n",
    "    def weight_matrix(self, pos_inx, batch_size, seq_len):\n",
    "        pos_inx = pos_inx.cpu().numpy()\n",
    "        weight = [[] for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            for j in range(pos_inx[i][0]):\n",
    "                relative_pos = pos_inx[i][0] - j\n",
    "                aspect_len = pos_inx[i][1] - pos_inx[i][0] + 1\n",
    "                sentence_len = seq_len - aspect_len\n",
    "                weight[i].append(1 - relative_pos / sentence_len)\n",
    "            for j in range(pos_inx[i][0], pos_inx[i][1] + 1):\n",
    "                weight[i].append(0)\n",
    "            for j in range(pos_inx[i][1] + 1, seq_len):\n",
    "                relative_pos = j - pos_inx[i][1]\n",
    "                aspect_len = pos_inx[i][1] - pos_inx[i][0] + 1\n",
    "                sentence_len = seq_len - aspect_len\n",
    "                weight[i].append(1 - relative_pos / sentence_len)\n",
    "        weight = torch.tensor(weight)\n",
    "        return weight\n",
    "\n",
    "class AlignmentMatrix(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(AlignmentMatrix, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.w_u = nn.Parameter(torch.Tensor(6*opt.hidden_dim, 1))\n",
    "\n",
    "    def forward(self, batch_size, ctx, asp):\n",
    "        ctx_len = ctx.size(1)\n",
    "        asp_len = asp.size(1)\n",
    "        alignment_mat = torch.zeros(batch_size, ctx_len, asp_len).to(self.opt.device)\n",
    "        ctx_chunks = ctx.chunk(ctx_len, dim=1)\n",
    "        asp_chunks = asp.chunk(asp_len, dim=1)\n",
    "        for i, ctx_chunk in enumerate(ctx_chunks):\n",
    "            for j, asp_chunk in enumerate(asp_chunks):\n",
    "                feat = torch.cat([ctx_chunk, asp_chunk, ctx_chunk*asp_chunk], dim=2) # batch_size x 1 x 6*hidden_dim \n",
    "                alignment_mat[:, i, j] = feat.matmul(self.w_u.expand(batch_size, -1, -1)).squeeze(-1).squeeze(-1) \n",
    "        return alignment_mat\n",
    "\n",
    "class MGAN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, opt):\n",
    "        super(MGAN, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.ctx_lstm = DynamicLSTM(opt.embed_dim, opt.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.asp_lstm = DynamicLSTM(opt.embed_dim, opt.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.location = LocationEncoding(opt)\n",
    "        self.w_a2c = nn.Parameter(torch.Tensor(2*opt.hidden_dim, 2*opt.hidden_dim))\n",
    "        self.w_c2a = nn.Parameter(torch.Tensor(2*opt.hidden_dim, 2*opt.hidden_dim))\n",
    "        self.alignment = AlignmentMatrix(opt)\n",
    "        self.dense = nn.Linear(8*opt.hidden_dim, opt.polarities_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_raw_indices = inputs[0] # batch_size x seq_len\n",
    "        aspect_indices = inputs[1] \n",
    "        text_left_indices= inputs[2]\n",
    "        batch_size = text_raw_indices.size(0)\n",
    "        ctx_len = torch.sum(text_raw_indices != 0, dim=1)\n",
    "        asp_len = torch.sum(aspect_indices != 0, dim=1)\n",
    "        left_len = torch.sum(text_left_indices != 0, dim=-1)\n",
    "        aspect_in_text = torch.cat([left_len.unsqueeze(-1), (left_len+asp_len-1).unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        ctx = self.embed(text_raw_indices) # batch_size x seq_len x embed_dim\n",
    "        asp = self.embed(aspect_indices) # batch_size x seq_len x embed_dim\n",
    "\n",
    "        ctx_out, (_, _) = self.ctx_lstm(ctx, ctx_len) \n",
    "        ctx_out = self.location(ctx_out, aspect_in_text) # batch_size x (ctx)seq_len x 2*hidden_dim\n",
    "        ctx_pool = torch.sum(ctx_out, dim=1)\n",
    "        ctx_pool = torch.div(ctx_pool, ctx_len.float().unsqueeze(-1)).unsqueeze(-1) # batch_size x 2*hidden_dim x 1\n",
    "\n",
    "        asp_out, (_, _) = self.asp_lstm(asp, asp_len) # batch_size x (asp)seq_len x 2*hidden_dim\n",
    "        asp_pool = torch.sum(asp_out, dim=1)\n",
    "        asp_pool = torch.div(asp_pool, asp_len.float().unsqueeze(-1)).unsqueeze(-1) # batch_size x 2*hidden_dim x 1\n",
    "\n",
    "        alignment_mat = self.alignment(batch_size, ctx_out, asp_out) # batch_size x (ctx)seq_len x (asp)seq_len\n",
    "        # batch_size x 2*hidden_dim\n",
    "        f_asp2ctx = torch.matmul(ctx_out.transpose(1, 2), F.softmax(alignment_mat.max(2, keepdim=True)[0], dim=1)).squeeze(-1)\n",
    "        f_ctx2asp = torch.matmul(F.softmax(alignment_mat.max(1, keepdim=True)[0], dim=2), asp_out).transpose(1, 2).squeeze(-1) \n",
    "\n",
    "        c_asp2ctx_alpha = F.softmax(ctx_out.matmul(self.w_a2c.expand(batch_size, -1, -1)).matmul(asp_pool), dim=1)\n",
    "        c_asp2ctx = torch.matmul(ctx_out.transpose(1, 2), c_asp2ctx_alpha).squeeze(-1)\n",
    "        c_ctx2asp_alpha = F.softmax(asp_out.matmul(self.w_c2a.expand(batch_size, -1, -1)).matmul(ctx_pool), dim=1)\n",
    "        c_ctx2asp = torch.matmul(asp_out.transpose(1, 2), c_ctx2asp_alpha).squeeze(-1)\n",
    "\n",
    "        feat = torch.cat([c_asp2ctx, f_asp2ctx, f_ctx2asp, c_ctx2asp], dim=1)\n",
    "        out = self.dense(feat) # bathc_size x polarity_dim\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# file: train.py\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "\n",
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "        tokenizer = build_tokenizer(\n",
    "            fnames=[opt.dataset_file['train'], opt.dataset_file['test']],\n",
    "            max_seq_len=opt.max_seq_len,\n",
    "            dat_fname='{0}_tokenizer.dat'.format(opt.dataset))\n",
    "        embedding_matrix = build_embedding_matrix(\n",
    "            word2idx=tokenizer.word2idx,\n",
    "            embed_dim=opt.embed_dim,\n",
    "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))\n",
    "        self.model = opt.model_class(embedding_matrix, opt).to(opt.device)\n",
    "        self.trainset = ABSADataset(opt.dataset_file['train'], tokenizer)\n",
    "        self.testset = ABSADataset(opt.dataset_file['test'], tokenizer)\n",
    "        assert 0 <= opt.valset_ratio < 1\n",
    "        if opt.valset_ratio > 0:\n",
    "            valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
    "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
    "        else:\n",
    "            self.valset = self.testset\n",
    "\n",
    "        if opt.device.type == 'cuda':\n",
    "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
    "        self._print_args()\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        logger.info('> training parameters:')\n",
    "        for arg in vars(self.opt):\n",
    "            logger.info('> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad:\n",
    "                    if len(p.shape) > 1:\n",
    "                        self.opt.initializer(p)\n",
    "                    else:\n",
    "                        stdv = 1. / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
    "        max_val_acc = 0\n",
    "        max_val_f1 = 0\n",
    "        global_step = 0\n",
    "        path = None\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            logger.info(' ' * 100)\n",
    "            logger.info('epoch: {}'.format(epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            # switch model to training mode\n",
    "            self.model.train()\n",
    "            for i_batch, sample_batched in enumerate(train_data_loader):\n",
    "                global_step += 1\n",
    "                # clear gradient accumulators\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = [sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                outputs = self.model(inputs)\n",
    "                targets = sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                loss_total += loss.item() * len(outputs)\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n",
    "            logger.info('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                if not os.path.exists('state_dict'):\n",
    "                    os.mkdir('state_dict')\n",
    "                path = 'state_dict/{0}_{1}_val_acc{2}'.format(self.opt.model_name, self.opt.dataset, round(val_acc, 4))\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "                logger.info('>> saved: {}'.format(path))\n",
    "            if val_f1 > max_val_f1:\n",
    "                max_val_f1 = val_f1\n",
    "\n",
    "        return path\n",
    "\n",
    "    def _evaluate_acc_f1(self, data_loader):\n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "                t_inputs = [t_sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                t_targets = t_sample_batched['polarity'].to(self.opt.device)\n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "        return acc, f1\n",
    "\n",
    "    def run(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = self.opt.optimizer(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "\n",
    "        self._reset_params()\n",
    "        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        self.model.eval()\n",
    "        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n",
    "        logger.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = {\n",
    "    'mgan': MGAN,\n",
    "    'lstm': LSTM,\n",
    "}\n",
    "dataset_files = {\n",
    "    'twitter': {\n",
    "        'train': './datasets/acl-14-short-data/train.raw',\n",
    "        'test': './datasets/acl-14-short-data/test.raw'\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'train': './datasets/semeval14/Restaurants_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "    }\n",
    "}\n",
    "input_colses = {\n",
    "    'lstm': ['text_raw_indices'],\n",
    "    'mgan': ['text_raw_indices', 'aspect_indices', 'text_left_indices'],\n",
    "}\n",
    "initializers = {\n",
    "    'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "}\n",
    "optimizers = {\n",
    "    'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "    'adam': torch.optim.Adam,  # default lr=0.001\n",
    "    'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "    'sgd': torch.optim.SGD,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, model_class, dataset_file, inputs_cols, initializer, \n",
    "                 optimizer, model_name, dataset, learning_rate, num_epoch):\n",
    "        self.model_class = model_class\n",
    "        self.dataset_file = dataset_file\n",
    "        self.inputs_cols = inputs_cols\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        \n",
    "        self.dropout = 0.1\n",
    "        self.l2reg = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.log_step = 5\n",
    "        self.embed_dim = 300\n",
    "        self.hidden_dim = 300\n",
    "        self.max_seq_len = 80\n",
    "        self.polarities_dim = 3\n",
    "        self.valset_ratio = 0\n",
    "        self.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - laptop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: laptop_tokenizer.dat\n",
      "loading embedding_matrix: 300_laptop_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.LSTM'>\n",
      "> dataset_file: {'train': './datasets/semeval14/Laptops_Train.xml.seg', 'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'}\n",
      "> inputs_cols: ['text_raw_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a15a45b70>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: lstm\n",
      "> dataset: laptop\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> hops: 3\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.0629, acc: 0.4906\n",
      "loss: 1.0317, acc: 0.4906\n",
      "loss: 1.0269, acc: 0.4906\n",
      "loss: 1.0190, acc: 0.5008\n",
      "loss: 1.0059, acc: 0.5069\n",
      "loss: 0.9876, acc: 0.5234\n",
      "loss: 0.9760, acc: 0.5362\n",
      "> val_acc: 0.5878, val_f1: 0.4182\n",
      ">> saved: state_dict/lstm_laptop_val_acc0.5878\n",
      "                                                                                                    \n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8096, acc: 0.6667\n",
      "loss: 0.8166, acc: 0.6621\n",
      "loss: 0.8093, acc: 0.6611\n",
      "loss: 0.8063, acc: 0.6554\n",
      "loss: 0.8071, acc: 0.6522\n",
      "loss: 0.8058, acc: 0.6557\n",
      "loss: 0.8041, acc: 0.6600\n",
      "> val_acc: 0.6097, val_f1: 0.4695\n",
      ">> saved: state_dict/lstm_laptop_val_acc0.6097\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.7013, acc: 0.7188\n",
      "loss: 0.7512, acc: 0.7031\n",
      "loss: 0.7357, acc: 0.7102\n",
      "loss: 0.7461, acc: 0.6963\n",
      "loss: 0.7416, acc: 0.6994\n",
      "loss: 0.7361, acc: 0.7031\n",
      "loss: 0.7374, acc: 0.7036\n",
      "loss: 0.7496, acc: 0.6984\n",
      "> val_acc: 0.6630, val_f1: 0.5667\n",
      ">> saved: state_dict/lstm_laptop_val_acc0.663\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.7309, acc: 0.6914\n",
      "loss: 0.7338, acc: 0.6875\n",
      "loss: 0.7633, acc: 0.6786\n",
      "loss: 0.7403, acc: 0.6957\n",
      "loss: 0.7360, acc: 0.6979\n",
      "loss: 0.7297, acc: 0.7010\n",
      "loss: 0.7271, acc: 0.7045\n",
      "> val_acc: 0.6176, val_f1: 0.4430\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.7902, acc: 0.6562\n",
      "loss: 0.7605, acc: 0.6897\n",
      "loss: 0.7296, acc: 0.7122\n",
      "loss: 0.7069, acc: 0.7188\n",
      "loss: 0.7235, acc: 0.7102\n",
      "loss: 0.7441, acc: 0.6950\n",
      "loss: 0.7626, acc: 0.6787\n",
      "loss: 0.7593, acc: 0.6791\n",
      "> val_acc: 0.6160, val_f1: 0.4419\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.7406, acc: 0.6656\n",
      "loss: 0.7393, acc: 0.6797\n",
      "loss: 0.7441, acc: 0.6833\n",
      "loss: 0.7220, acc: 0.6977\n",
      "loss: 0.7146, acc: 0.7037\n",
      "loss: 0.7211, acc: 0.6995\n",
      "loss: 0.7138, acc: 0.7036\n",
      "> val_acc: 0.6285, val_f1: 0.4599\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.8251, acc: 0.6302\n",
      "loss: 0.7316, acc: 0.7070\n",
      "loss: 0.7094, acc: 0.7200\n",
      "loss: 0.6888, acc: 0.7240\n",
      "loss: 0.6735, acc: 0.7296\n",
      "loss: 0.6716, acc: 0.7333\n",
      "loss: 0.6959, acc: 0.7183\n",
      "> val_acc: 0.6599, val_f1: 0.5488\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.5949, acc: 0.7969\n",
      "loss: 0.6916, acc: 0.7240\n",
      "loss: 0.7051, acc: 0.7088\n",
      "loss: 0.6915, acc: 0.7178\n",
      "loss: 0.6790, acc: 0.7314\n",
      "loss: 0.6904, acc: 0.7266\n",
      "loss: 0.6810, acc: 0.7308\n",
      "loss: 0.6729, acc: 0.7331\n",
      "> val_acc: 0.6348, val_f1: 0.4973\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.7237, acc: 0.6914\n",
      "loss: 0.7228, acc: 0.7049\n",
      "loss: 0.7067, acc: 0.7143\n",
      "loss: 0.7041, acc: 0.7122\n",
      "loss: 0.6841, acc: 0.7220\n",
      "loss: 0.6698, acc: 0.7290\n",
      "loss: 0.6607, acc: 0.7312\n",
      "> val_acc: 0.6630, val_f1: 0.5810\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.5053, acc: 0.8203\n",
      "loss: 0.5895, acc: 0.7723\n",
      "loss: 0.6349, acc: 0.7435\n",
      "loss: 0.6419, acc: 0.7344\n",
      "loss: 0.6656, acc: 0.7173\n",
      "loss: 0.6622, acc: 0.7234\n",
      "loss: 0.6646, acc: 0.7246\n",
      "loss: 0.6584, acc: 0.7315\n",
      "> val_acc: 0.6771, val_f1: 0.6137\n",
      ">> saved: state_dict/lstm_laptop_val_acc0.6771\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.6287, acc: 0.7594\n",
      "loss: 0.5744, acc: 0.7906\n",
      "loss: 0.5933, acc: 0.7823\n",
      "loss: 0.6171, acc: 0.7688\n",
      "loss: 0.6153, acc: 0.7731\n",
      "loss: 0.6277, acc: 0.7630\n",
      "loss: 0.6167, acc: 0.7683\n",
      "> val_acc: 0.6520, val_f1: 0.5388\n",
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.6618, acc: 0.7240\n",
      "loss: 0.6011, acc: 0.7676\n",
      "loss: 0.5936, acc: 0.7752\n",
      "loss: 0.5956, acc: 0.7726\n",
      "loss: 0.6000, acc: 0.7724\n",
      "loss: 0.6132, acc: 0.7656\n",
      "loss: 0.6074, acc: 0.7708\n",
      "> val_acc: 0.6693, val_f1: 0.6117\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.5294, acc: 0.8125\n",
      "loss: 0.5762, acc: 0.7760\n",
      "loss: 0.5975, acc: 0.7713\n",
      "loss: 0.6078, acc: 0.7695\n",
      "loss: 0.5959, acc: 0.7753\n",
      "loss: 0.6109, acc: 0.7662\n",
      "loss: 0.6274, acc: 0.7571\n",
      "loss: 0.6362, acc: 0.7517\n",
      "> val_acc: 0.6693, val_f1: 0.5922\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.5402, acc: 0.8281\n",
      "loss: 0.5901, acc: 0.7812\n",
      "loss: 0.5859, acc: 0.7723\n",
      "loss: 0.6061, acc: 0.7689\n",
      "loss: 0.6108, acc: 0.7617\n",
      "loss: 0.6063, acc: 0.7662\n",
      "loss: 0.6132, acc: 0.7615\n",
      "> val_acc: 0.6818, val_f1: 0.6297\n",
      ">> saved: state_dict/lstm_laptop_val_acc0.6818\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.6536, acc: 0.7344\n",
      "loss: 0.6226, acc: 0.7545\n",
      "loss: 0.6097, acc: 0.7643\n",
      "loss: 0.6198, acc: 0.7610\n",
      "loss: 0.6159, acc: 0.7642\n",
      "loss: 0.6225, acc: 0.7598\n",
      "loss: 0.6213, acc: 0.7568\n",
      "loss: 0.6056, acc: 0.7637\n",
      "> val_acc: 0.6505, val_f1: 0.5391\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.6023, acc: 0.7562\n",
      "loss: 0.5728, acc: 0.7688\n",
      "loss: 0.5636, acc: 0.7750\n",
      "loss: 0.5653, acc: 0.7766\n",
      "loss: 0.5956, acc: 0.7662\n",
      "loss: 0.6009, acc: 0.7656\n",
      "loss: 0.6118, acc: 0.7603\n",
      "> val_acc: 0.6567, val_f1: 0.6019\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.7120, acc: 0.7031\n",
      "loss: 0.6604, acc: 0.7324\n",
      "loss: 0.6100, acc: 0.7656\n",
      "loss: 0.6159, acc: 0.7639\n",
      "loss: 0.5995, acc: 0.7643\n",
      "loss: 0.5942, acc: 0.7690\n",
      "loss: 0.5934, acc: 0.7694\n",
      "> val_acc: 0.6583, val_f1: 0.5815\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.5276, acc: 0.8438\n",
      "loss: 0.5650, acc: 0.7865\n",
      "loss: 0.5531, acc: 0.7912\n",
      "loss: 0.5365, acc: 0.8018\n",
      "loss: 0.5416, acc: 0.7999\n",
      "loss: 0.5541, acc: 0.7933\n",
      "loss: 0.5586, acc: 0.7888\n",
      "loss: 0.5578, acc: 0.7899\n",
      "> val_acc: 0.6567, val_f1: 0.5820\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.5661, acc: 0.7852\n",
      "loss: 0.5171, acc: 0.8056\n",
      "loss: 0.5358, acc: 0.8047\n",
      "loss: 0.5369, acc: 0.8051\n",
      "loss: 0.5279, acc: 0.8099\n",
      "loss: 0.5360, acc: 0.8028\n",
      "loss: 0.5361, acc: 0.8024\n",
      "> val_acc: 0.6818, val_f1: 0.6226\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.4829, acc: 0.7969\n",
      "loss: 0.5096, acc: 0.7969\n",
      "loss: 0.4929, acc: 0.8086\n",
      "loss: 0.5253, acc: 0.7941\n",
      "loss: 0.5495, acc: 0.7834\n",
      "loss: 0.5536, acc: 0.7789\n",
      "loss: 0.5553, acc: 0.7842\n",
      "loss: 0.5571, acc: 0.7831\n",
      "> val_acc: 0.6646, val_f1: 0.5559\n",
      ">> test_acc: 0.6818, test_f1: 0.6297\n"
     ]
    }
   ],
   "source": [
    "model_name = 'lstm'\n",
    "dataset = 'laptop' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_lstm = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_lstm)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - restaurant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: restaurant_tokenizer.dat\n",
      "loading embedding_matrix: 300_restaurant_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.LSTM'>\n",
      "> dataset_file: {'train': './datasets/semeval14/Restaurants_Train.xml.seg', 'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'}\n",
      "> inputs_cols: ['text_raw_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a18039ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: lstm\n",
      "> dataset: restaurant\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 0.9327, acc: 0.6469\n",
      "loss: 0.9501, acc: 0.6234\n",
      "loss: 0.9588, acc: 0.6219\n",
      "loss: 0.9486, acc: 0.6219\n",
      "loss: 0.9348, acc: 0.6219\n",
      "loss: 0.9356, acc: 0.6146\n",
      "loss: 0.9253, acc: 0.6188\n",
      "loss: 0.9194, acc: 0.6156\n",
      "loss: 0.9085, acc: 0.6156\n",
      "loss: 0.8990, acc: 0.6191\n",
      "loss: 0.8905, acc: 0.6199\n",
      "> val_acc: 0.6661, val_f1: 0.3305\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.6661\n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "loss: 0.7883, acc: 0.6510\n",
      "loss: 0.7440, acc: 0.6855\n",
      "loss: 0.7544, acc: 0.6791\n",
      "loss: 0.7541, acc: 0.6866\n",
      "loss: 0.7497, acc: 0.6848\n",
      "loss: 0.7490, acc: 0.6836\n",
      "loss: 0.7467, acc: 0.6847\n",
      "loss: 0.7442, acc: 0.6854\n",
      "loss: 0.7442, acc: 0.6828\n",
      "loss: 0.7444, acc: 0.6849\n",
      "loss: 0.7375, acc: 0.6887\n",
      "> val_acc: 0.7286, val_f1: 0.4977\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.7286\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.7677, acc: 0.6719\n",
      "loss: 0.7355, acc: 0.7031\n",
      "loss: 0.7025, acc: 0.7202\n",
      "loss: 0.7237, acc: 0.7021\n",
      "loss: 0.7291, acc: 0.6875\n",
      "loss: 0.7256, acc: 0.6941\n",
      "loss: 0.7161, acc: 0.7001\n",
      "loss: 0.7196, acc: 0.6997\n",
      "loss: 0.7130, acc: 0.7039\n",
      "loss: 0.7143, acc: 0.7045\n",
      "loss: 0.7157, acc: 0.7010\n",
      "loss: 0.7183, acc: 0.6959\n",
      "> val_acc: 0.7366, val_f1: 0.5669\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.7366\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.7246, acc: 0.6836\n",
      "loss: 0.6990, acc: 0.7101\n",
      "loss: 0.7061, acc: 0.7031\n",
      "loss: 0.6686, acc: 0.7278\n",
      "loss: 0.6734, acc: 0.7207\n",
      "loss: 0.6734, acc: 0.7214\n",
      "loss: 0.6785, acc: 0.7220\n",
      "loss: 0.6760, acc: 0.7216\n",
      "loss: 0.6782, acc: 0.7212\n",
      "loss: 0.6813, acc: 0.7184\n",
      "loss: 0.6848, acc: 0.7150\n",
      "> val_acc: 0.7402, val_f1: 0.5868\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.7402\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.6851, acc: 0.7422\n",
      "loss: 0.6648, acc: 0.6987\n",
      "loss: 0.6821, acc: 0.6927\n",
      "loss: 0.6696, acc: 0.7096\n",
      "loss: 0.6638, acc: 0.7131\n",
      "loss: 0.6765, acc: 0.7089\n",
      "loss: 0.6875, acc: 0.7090\n",
      "loss: 0.6871, acc: 0.7065\n",
      "loss: 0.6876, acc: 0.7094\n",
      "loss: 0.6835, acc: 0.7108\n",
      "loss: 0.6749, acc: 0.7178\n",
      "loss: 0.6770, acc: 0.7170\n",
      "> val_acc: 0.7295, val_f1: 0.5285\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.5867, acc: 0.7344\n",
      "loss: 0.6313, acc: 0.7328\n",
      "loss: 0.6164, acc: 0.7490\n",
      "loss: 0.6405, acc: 0.7422\n",
      "loss: 0.6393, acc: 0.7438\n",
      "loss: 0.6477, acc: 0.7359\n",
      "loss: 0.6541, acc: 0.7330\n",
      "loss: 0.6554, acc: 0.7332\n",
      "loss: 0.6514, acc: 0.7354\n",
      "loss: 0.6620, acc: 0.7284\n",
      "loss: 0.6678, acc: 0.7256\n",
      "> val_acc: 0.7232, val_f1: 0.5162\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.6643, acc: 0.7135\n",
      "loss: 0.6664, acc: 0.7246\n",
      "loss: 0.6644, acc: 0.7236\n",
      "loss: 0.6786, acc: 0.7153\n",
      "loss: 0.6790, acc: 0.7154\n",
      "loss: 0.6934, acc: 0.7081\n",
      "loss: 0.6785, acc: 0.7183\n",
      "loss: 0.6723, acc: 0.7225\n",
      "loss: 0.6692, acc: 0.7227\n",
      "loss: 0.6706, acc: 0.7227\n",
      "loss: 0.6773, acc: 0.7170\n",
      "> val_acc: 0.7384, val_f1: 0.5402\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.5120, acc: 0.8125\n",
      "loss: 0.6213, acc: 0.7240\n",
      "loss: 0.6197, acc: 0.7358\n",
      "loss: 0.6361, acc: 0.7344\n",
      "loss: 0.6608, acc: 0.7210\n",
      "loss: 0.6621, acc: 0.7194\n",
      "loss: 0.6655, acc: 0.7198\n",
      "loss: 0.6641, acc: 0.7209\n",
      "loss: 0.6636, acc: 0.7218\n",
      "loss: 0.6673, acc: 0.7211\n",
      "loss: 0.6681, acc: 0.7224\n",
      "loss: 0.6677, acc: 0.7210\n",
      "> val_acc: 0.7455, val_f1: 0.5989\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.7455\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.6995, acc: 0.7344\n",
      "loss: 0.6674, acc: 0.7517\n",
      "loss: 0.6638, acc: 0.7422\n",
      "loss: 0.6492, acc: 0.7426\n",
      "loss: 0.6532, acc: 0.7357\n",
      "loss: 0.6572, acc: 0.7349\n",
      "loss: 0.6490, acc: 0.7385\n",
      "loss: 0.6410, acc: 0.7408\n",
      "loss: 0.6571, acc: 0.7351\n",
      "loss: 0.6527, acc: 0.7363\n",
      "loss: 0.6550, acc: 0.7335\n",
      "> val_acc: 0.7455, val_f1: 0.5697\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6983, acc: 0.6953\n",
      "loss: 0.6749, acc: 0.7321\n",
      "loss: 0.6518, acc: 0.7435\n",
      "loss: 0.6488, acc: 0.7408\n",
      "loss: 0.6370, acc: 0.7507\n",
      "loss: 0.6276, acc: 0.7512\n",
      "loss: 0.6382, acc: 0.7461\n",
      "loss: 0.6355, acc: 0.7496\n",
      "loss: 0.6365, acc: 0.7470\n",
      "loss: 0.6324, acc: 0.7480\n",
      "loss: 0.6320, acc: 0.7488\n",
      "loss: 0.6390, acc: 0.7464\n",
      "> val_acc: 0.7411, val_f1: 0.5967\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.6198, acc: 0.7406\n",
      "loss: 0.5855, acc: 0.7609\n",
      "loss: 0.5817, acc: 0.7667\n",
      "loss: 0.6208, acc: 0.7469\n",
      "loss: 0.6292, acc: 0.7419\n",
      "loss: 0.6308, acc: 0.7448\n",
      "loss: 0.6308, acc: 0.7464\n",
      "loss: 0.6308, acc: 0.7445\n",
      "loss: 0.6381, acc: 0.7417\n",
      "loss: 0.6504, acc: 0.7347\n",
      "loss: 0.6496, acc: 0.7324\n",
      "> val_acc: 0.7321, val_f1: 0.5348\n",
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.6302, acc: 0.7656\n",
      "loss: 0.6441, acc: 0.7480\n",
      "loss: 0.6551, acc: 0.7356\n",
      "loss: 0.6616, acc: 0.7326\n",
      "loss: 0.6658, acc: 0.7303\n",
      "loss: 0.6628, acc: 0.7288\n",
      "loss: 0.6574, acc: 0.7320\n",
      "loss: 0.6527, acc: 0.7377\n",
      "loss: 0.6435, acc: 0.7431\n",
      "loss: 0.6400, acc: 0.7428\n",
      "loss: 0.6318, acc: 0.7479\n",
      "> val_acc: 0.7339, val_f1: 0.5920\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.6314, acc: 0.7500\n",
      "loss: 0.6277, acc: 0.7318\n",
      "loss: 0.6446, acc: 0.7230\n",
      "loss: 0.6457, acc: 0.7354\n",
      "loss: 0.6292, acc: 0.7515\n",
      "loss: 0.6249, acc: 0.7512\n",
      "loss: 0.6205, acc: 0.7515\n",
      "loss: 0.6158, acc: 0.7522\n",
      "loss: 0.6245, acc: 0.7481\n",
      "loss: 0.6255, acc: 0.7449\n",
      "loss: 0.6264, acc: 0.7433\n",
      "loss: 0.6290, acc: 0.7422\n",
      "> val_acc: 0.7366, val_f1: 0.5309\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.6365, acc: 0.7539\n",
      "loss: 0.5840, acc: 0.7674\n",
      "loss: 0.5822, acc: 0.7757\n",
      "loss: 0.5779, acc: 0.7771\n",
      "loss: 0.5914, acc: 0.7637\n",
      "loss: 0.5943, acc: 0.7608\n",
      "loss: 0.5977, acc: 0.7606\n",
      "loss: 0.6076, acc: 0.7564\n",
      "loss: 0.6054, acc: 0.7553\n",
      "loss: 0.6094, acc: 0.7532\n",
      "loss: 0.6177, acc: 0.7497\n",
      "> val_acc: 0.7330, val_f1: 0.5527\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.5641, acc: 0.7500\n",
      "loss: 0.6050, acc: 0.7522\n",
      "loss: 0.5977, acc: 0.7604\n",
      "loss: 0.5971, acc: 0.7665\n",
      "loss: 0.6049, acc: 0.7614\n",
      "loss: 0.6111, acc: 0.7587\n",
      "loss: 0.6143, acc: 0.7573\n",
      "loss: 0.6095, acc: 0.7568\n",
      "loss: 0.6027, acc: 0.7589\n",
      "loss: 0.6031, acc: 0.7600\n",
      "loss: 0.6055, acc: 0.7596\n",
      "loss: 0.6094, acc: 0.7558\n",
      "> val_acc: 0.7473, val_f1: 0.6139\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.7473\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.6587, acc: 0.7281\n",
      "loss: 0.6032, acc: 0.7438\n",
      "loss: 0.5946, acc: 0.7521\n",
      "loss: 0.5935, acc: 0.7586\n",
      "loss: 0.5855, acc: 0.7594\n",
      "loss: 0.5947, acc: 0.7542\n",
      "loss: 0.6106, acc: 0.7460\n",
      "loss: 0.6130, acc: 0.7496\n",
      "loss: 0.6093, acc: 0.7528\n",
      "loss: 0.6050, acc: 0.7559\n",
      "loss: 0.6083, acc: 0.7551\n",
      "> val_acc: 0.7536, val_f1: 0.6365\n",
      ">> saved: state_dict/lstm_restaurant_val_acc0.7536\n",
      "                                                                                                    \n",
      "epoch: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5457, acc: 0.7552\n",
      "loss: 0.5585, acc: 0.7656\n",
      "loss: 0.5793, acc: 0.7560\n",
      "loss: 0.5877, acc: 0.7543\n",
      "loss: 0.5973, acc: 0.7527\n",
      "loss: 0.6105, acc: 0.7455\n",
      "loss: 0.6024, acc: 0.7500\n",
      "loss: 0.5981, acc: 0.7537\n",
      "loss: 0.6043, acc: 0.7525\n",
      "loss: 0.6071, acc: 0.7487\n",
      "loss: 0.6066, acc: 0.7503\n",
      "> val_acc: 0.7473, val_f1: 0.6219\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.6716, acc: 0.6250\n",
      "loss: 0.6070, acc: 0.7500\n",
      "loss: 0.6067, acc: 0.7543\n",
      "loss: 0.5896, acc: 0.7598\n",
      "loss: 0.5867, acc: 0.7641\n",
      "loss: 0.5836, acc: 0.7614\n",
      "loss: 0.5855, acc: 0.7626\n",
      "loss: 0.5856, acc: 0.7635\n",
      "loss: 0.5871, acc: 0.7607\n",
      "loss: 0.5934, acc: 0.7582\n",
      "loss: 0.5910, acc: 0.7586\n",
      "loss: 0.5890, acc: 0.7592\n",
      "> val_acc: 0.7286, val_f1: 0.5719\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.6038, acc: 0.7578\n",
      "loss: 0.5579, acc: 0.7882\n",
      "loss: 0.5532, acc: 0.7857\n",
      "loss: 0.5553, acc: 0.7837\n",
      "loss: 0.5624, acc: 0.7812\n",
      "loss: 0.5699, acc: 0.7726\n",
      "loss: 0.5777, acc: 0.7688\n",
      "loss: 0.5796, acc: 0.7672\n",
      "loss: 0.5892, acc: 0.7621\n",
      "loss: 0.5861, acc: 0.7653\n",
      "loss: 0.5825, acc: 0.7691\n",
      "> val_acc: 0.7455, val_f1: 0.6279\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.4500, acc: 0.8828\n",
      "loss: 0.4951, acc: 0.8214\n",
      "loss: 0.5190, acc: 0.8112\n",
      "loss: 0.5480, acc: 0.7932\n",
      "loss: 0.5441, acc: 0.7912\n",
      "loss: 0.5571, acc: 0.7812\n",
      "loss: 0.5594, acc: 0.7842\n",
      "loss: 0.5591, acc: 0.7796\n",
      "loss: 0.5648, acc: 0.7742\n",
      "loss: 0.5749, acc: 0.7693\n",
      "loss: 0.5848, acc: 0.7647\n",
      "loss: 0.5864, acc: 0.7647\n",
      "> val_acc: 0.7473, val_f1: 0.6161\n",
      ">> test_acc: 0.7536, test_f1: 0.6365\n"
     ]
    }
   ],
   "source": [
    "model_name = 'lstm'\n",
    "dataset = 'restaurant' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_lstm = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_lstm)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: twitter_tokenizer.dat\n",
      "loading embedding_matrix: 300_twitter_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.LSTM'>\n",
      "> dataset_file: {'train': './datasets/acl-14-short-data/train.raw', 'test': './datasets/acl-14-short-data/test.raw'}\n",
      "> inputs_cols: ['text_raw_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a18039ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: lstm\n",
      "> dataset: twitter\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.0440, acc: 0.4719\n",
      "loss: 1.0530, acc: 0.4719\n",
      "loss: 1.0364, acc: 0.4948\n",
      "loss: 1.0413, acc: 0.4914\n",
      "loss: 1.0330, acc: 0.4950\n",
      "loss: 1.0308, acc: 0.4927\n",
      "loss: 1.0265, acc: 0.4942\n",
      "loss: 1.0173, acc: 0.5012\n",
      "loss: 1.0141, acc: 0.5017\n",
      "loss: 1.0062, acc: 0.5075\n",
      "loss: 1.0026, acc: 0.5071\n",
      "loss: 0.9961, acc: 0.5091\n",
      "loss: 0.9923, acc: 0.5125\n",
      "loss: 0.9884, acc: 0.5156\n",
      "loss: 0.9849, acc: 0.5179\n",
      "loss: 0.9826, acc: 0.5211\n",
      "loss: 0.9777, acc: 0.5270\n",
      "loss: 0.9727, acc: 0.5267\n",
      "loss: 0.9683, acc: 0.5317\n",
      "> val_acc: 0.6055, val_f1: 0.5571\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6055\n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "loss: 0.8126, acc: 0.5703\n",
      "loss: 0.8050, acc: 0.6049\n",
      "loss: 0.8262, acc: 0.6016\n",
      "loss: 0.8558, acc: 0.5836\n",
      "loss: 0.8683, acc: 0.5824\n",
      "loss: 0.8703, acc: 0.5862\n",
      "loss: 0.8768, acc: 0.5845\n",
      "loss: 0.8752, acc: 0.5849\n",
      "loss: 0.8726, acc: 0.5878\n",
      "loss: 0.8734, acc: 0.5878\n",
      "loss: 0.8724, acc: 0.5916\n",
      "loss: 0.8759, acc: 0.5858\n",
      "loss: 0.8760, acc: 0.5862\n",
      "loss: 0.8778, acc: 0.5842\n",
      "loss: 0.8769, acc: 0.5862\n",
      "loss: 0.8746, acc: 0.5883\n",
      "loss: 0.8708, acc: 0.5897\n",
      "loss: 0.8679, acc: 0.5911\n",
      "loss: 0.8667, acc: 0.5932\n",
      "loss: 0.8668, acc: 0.5931\n",
      "> val_acc: 0.6113, val_f1: 0.5598\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6113\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.8748, acc: 0.5703\n",
      "loss: 0.8599, acc: 0.5885\n",
      "loss: 0.8485, acc: 0.6004\n",
      "loss: 0.8500, acc: 0.6044\n",
      "loss: 0.8368, acc: 0.6159\n",
      "loss: 0.8336, acc: 0.6185\n",
      "loss: 0.8310, acc: 0.6241\n",
      "loss: 0.8264, acc: 0.6254\n",
      "loss: 0.8281, acc: 0.6246\n",
      "loss: 0.8314, acc: 0.6173\n",
      "loss: 0.8317, acc: 0.6169\n",
      "loss: 0.8311, acc: 0.6178\n",
      "loss: 0.8301, acc: 0.6172\n",
      "loss: 0.8294, acc: 0.6184\n",
      "loss: 0.8259, acc: 0.6227\n",
      "loss: 0.8235, acc: 0.6256\n",
      "loss: 0.8218, acc: 0.6267\n",
      "loss: 0.8254, acc: 0.6243\n",
      "loss: 0.8265, acc: 0.6222\n",
      "> val_acc: 0.6214, val_f1: 0.5613\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6214\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.7961, acc: 0.6094\n",
      "loss: 0.8098, acc: 0.6328\n",
      "loss: 0.7988, acc: 0.6449\n",
      "loss: 0.8131, acc: 0.6299\n",
      "loss: 0.8157, acc: 0.6176\n",
      "loss: 0.8052, acc: 0.6256\n",
      "loss: 0.7972, acc: 0.6346\n",
      "loss: 0.8023, acc: 0.6363\n",
      "loss: 0.7982, acc: 0.6361\n",
      "loss: 0.8098, acc: 0.6270\n",
      "loss: 0.8145, acc: 0.6244\n",
      "loss: 0.8195, acc: 0.6214\n",
      "loss: 0.8188, acc: 0.6204\n",
      "loss: 0.8145, acc: 0.6219\n",
      "loss: 0.8124, acc: 0.6221\n",
      "loss: 0.8122, acc: 0.6236\n",
      "loss: 0.8162, acc: 0.6240\n",
      "loss: 0.8204, acc: 0.6225\n",
      "loss: 0.8172, acc: 0.6252\n",
      "loss: 0.8150, acc: 0.6260\n",
      "> val_acc: 0.6402, val_f1: 0.6197\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6402\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.8395, acc: 0.5938\n",
      "loss: 0.8032, acc: 0.6289\n",
      "loss: 0.7663, acc: 0.6550\n",
      "loss: 0.8060, acc: 0.6354\n",
      "loss: 0.8073, acc: 0.6318\n",
      "loss: 0.7911, acc: 0.6473\n",
      "loss: 0.7946, acc: 0.6491\n",
      "loss: 0.7970, acc: 0.6456\n",
      "loss: 0.7914, acc: 0.6504\n",
      "loss: 0.7869, acc: 0.6533\n",
      "loss: 0.7886, acc: 0.6515\n",
      "loss: 0.7850, acc: 0.6546\n",
      "loss: 0.7858, acc: 0.6543\n",
      "loss: 0.7888, acc: 0.6521\n",
      "loss: 0.7914, acc: 0.6505\n",
      "loss: 0.7948, acc: 0.6486\n",
      "loss: 0.7948, acc: 0.6482\n",
      "loss: 0.7934, acc: 0.6490\n",
      "loss: 0.7923, acc: 0.6507\n",
      "loss: 0.7953, acc: 0.6492\n",
      "> val_acc: 0.6445, val_f1: 0.6078\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6445\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.7561, acc: 0.6750\n",
      "loss: 0.7645, acc: 0.6672\n",
      "loss: 0.7690, acc: 0.6667\n",
      "loss: 0.7687, acc: 0.6687\n",
      "loss: 0.7732, acc: 0.6613\n",
      "loss: 0.7734, acc: 0.6526\n",
      "loss: 0.7771, acc: 0.6442\n",
      "loss: 0.7792, acc: 0.6449\n",
      "loss: 0.7798, acc: 0.6444\n",
      "loss: 0.7795, acc: 0.6450\n",
      "loss: 0.7832, acc: 0.6438\n",
      "loss: 0.7789, acc: 0.6492\n",
      "loss: 0.7762, acc: 0.6517\n",
      "loss: 0.7756, acc: 0.6516\n",
      "loss: 0.7736, acc: 0.6544\n",
      "loss: 0.7756, acc: 0.6545\n",
      "loss: 0.7777, acc: 0.6535\n",
      "loss: 0.7780, acc: 0.6540\n",
      "loss: 0.7779, acc: 0.6541\n",
      "> val_acc: 0.6532, val_f1: 0.6201\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6532\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.6990, acc: 0.7344\n",
      "loss: 0.7421, acc: 0.6853\n",
      "loss: 0.7585, acc: 0.6732\n",
      "loss: 0.7582, acc: 0.6719\n",
      "loss: 0.7667, acc: 0.6641\n",
      "loss: 0.7603, acc: 0.6678\n",
      "loss: 0.7563, acc: 0.6689\n",
      "loss: 0.7559, acc: 0.6689\n",
      "loss: 0.7657, acc: 0.6670\n",
      "loss: 0.7621, acc: 0.6705\n",
      "loss: 0.7592, acc: 0.6740\n",
      "loss: 0.7565, acc: 0.6743\n",
      "loss: 0.7587, acc: 0.6726\n",
      "loss: 0.7602, acc: 0.6700\n",
      "loss: 0.7638, acc: 0.6680\n",
      "loss: 0.7646, acc: 0.6668\n",
      "loss: 0.7666, acc: 0.6662\n",
      "loss: 0.7678, acc: 0.6649\n",
      "loss: 0.7676, acc: 0.6639\n",
      "loss: 0.7667, acc: 0.6649\n",
      "> val_acc: 0.6618, val_f1: 0.6144\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6618\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.7181, acc: 0.6992\n",
      "loss: 0.7292, acc: 0.6806\n",
      "loss: 0.7399, acc: 0.6696\n",
      "loss: 0.7533, acc: 0.6604\n",
      "loss: 0.7503, acc: 0.6654\n",
      "loss: 0.7511, acc: 0.6708\n",
      "loss: 0.7472, acc: 0.6737\n",
      "loss: 0.7509, acc: 0.6719\n",
      "loss: 0.7543, acc: 0.6722\n",
      "loss: 0.7606, acc: 0.6680\n",
      "loss: 0.7599, acc: 0.6681\n",
      "loss: 0.7587, acc: 0.6682\n",
      "loss: 0.7569, acc: 0.6689\n",
      "loss: 0.7572, acc: 0.6707\n",
      "loss: 0.7540, acc: 0.6744\n",
      "loss: 0.7517, acc: 0.6744\n",
      "loss: 0.7576, acc: 0.6709\n",
      "loss: 0.7590, acc: 0.6698\n",
      "loss: 0.7584, acc: 0.6697\n",
      "> val_acc: 0.6315, val_f1: 0.5695\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.6937, acc: 0.6719\n",
      "loss: 0.7588, acc: 0.6536\n",
      "loss: 0.7627, acc: 0.6577\n",
      "loss: 0.7509, acc: 0.6602\n",
      "loss: 0.7463, acc: 0.6659\n",
      "loss: 0.7389, acc: 0.6707\n",
      "loss: 0.7356, acc: 0.6794\n",
      "loss: 0.7358, acc: 0.6771\n",
      "loss: 0.7388, acc: 0.6768\n",
      "loss: 0.7396, acc: 0.6760\n",
      "loss: 0.7369, acc: 0.6783\n",
      "loss: 0.7353, acc: 0.6791\n",
      "loss: 0.7414, acc: 0.6742\n",
      "loss: 0.7438, acc: 0.6738\n",
      "loss: 0.7466, acc: 0.6710\n",
      "loss: 0.7485, acc: 0.6702\n",
      "loss: 0.7489, acc: 0.6696\n",
      "loss: 0.7492, acc: 0.6697\n",
      "loss: 0.7470, acc: 0.6703\n",
      "loss: 0.7462, acc: 0.6711\n",
      "> val_acc: 0.6662, val_f1: 0.6374\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6662\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6880, acc: 0.7031\n",
      "loss: 0.7129, acc: 0.6855\n",
      "loss: 0.7220, acc: 0.6911\n",
      "loss: 0.7147, acc: 0.7014\n",
      "loss: 0.7174, acc: 0.7024\n",
      "loss: 0.7244, acc: 0.6931\n",
      "loss: 0.7181, acc: 0.6979\n",
      "loss: 0.7172, acc: 0.6994\n",
      "loss: 0.7160, acc: 0.7002\n",
      "loss: 0.7168, acc: 0.7002\n",
      "loss: 0.7146, acc: 0.6990\n",
      "loss: 0.7216, acc: 0.6953\n",
      "loss: 0.7263, acc: 0.6902\n",
      "loss: 0.7268, acc: 0.6907\n",
      "loss: 0.7300, acc: 0.6890\n",
      "loss: 0.7359, acc: 0.6827\n",
      "loss: 0.7356, acc: 0.6830\n",
      "loss: 0.7348, acc: 0.6832\n",
      "loss: 0.7401, acc: 0.6794\n",
      "loss: 0.7441, acc: 0.6761\n",
      "> val_acc: 0.6633, val_f1: 0.6314\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.7913, acc: 0.6781\n",
      "loss: 0.7623, acc: 0.6937\n",
      "loss: 0.7573, acc: 0.6937\n",
      "loss: 0.7578, acc: 0.6953\n",
      "loss: 0.7466, acc: 0.6994\n",
      "loss: 0.7450, acc: 0.6948\n",
      "loss: 0.7420, acc: 0.6915\n",
      "loss: 0.7374, acc: 0.6937\n",
      "loss: 0.7321, acc: 0.6931\n",
      "loss: 0.7290, acc: 0.6931\n",
      "loss: 0.7307, acc: 0.6926\n",
      "loss: 0.7281, acc: 0.6937\n",
      "loss: 0.7284, acc: 0.6928\n",
      "loss: 0.7252, acc: 0.6924\n",
      "loss: 0.7293, acc: 0.6915\n",
      "loss: 0.7317, acc: 0.6906\n",
      "loss: 0.7312, acc: 0.6914\n",
      "loss: 0.7292, acc: 0.6920\n",
      "loss: 0.7303, acc: 0.6901\n",
      "> val_acc: 0.6358, val_f1: 0.6173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.6216, acc: 0.7578\n",
      "loss: 0.6860, acc: 0.7366\n",
      "loss: 0.7144, acc: 0.7083\n",
      "loss: 0.7118, acc: 0.7050\n",
      "loss: 0.7247, acc: 0.7010\n",
      "loss: 0.7217, acc: 0.7008\n",
      "loss: 0.7179, acc: 0.7017\n",
      "loss: 0.7163, acc: 0.7023\n",
      "loss: 0.7145, acc: 0.7024\n",
      "loss: 0.7075, acc: 0.7058\n",
      "loss: 0.7075, acc: 0.7043\n",
      "loss: 0.7052, acc: 0.7053\n",
      "loss: 0.7091, acc: 0.7021\n",
      "loss: 0.7120, acc: 0.7001\n",
      "loss: 0.7162, acc: 0.6986\n",
      "loss: 0.7163, acc: 0.6991\n",
      "loss: 0.7152, acc: 0.6997\n",
      "loss: 0.7209, acc: 0.6952\n",
      "loss: 0.7214, acc: 0.6948\n",
      "loss: 0.7228, acc: 0.6933\n",
      "> val_acc: 0.6575, val_f1: 0.6013\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.7464, acc: 0.6836\n",
      "loss: 0.7227, acc: 0.6823\n",
      "loss: 0.7247, acc: 0.6830\n",
      "loss: 0.7269, acc: 0.6891\n",
      "loss: 0.7140, acc: 0.6960\n",
      "loss: 0.7127, acc: 0.6977\n",
      "loss: 0.7100, acc: 0.6958\n",
      "loss: 0.7079, acc: 0.7003\n",
      "loss: 0.7088, acc: 0.6996\n",
      "loss: 0.7164, acc: 0.6939\n",
      "loss: 0.7131, acc: 0.6976\n",
      "loss: 0.7130, acc: 0.6970\n",
      "loss: 0.7118, acc: 0.6982\n",
      "loss: 0.7151, acc: 0.6966\n",
      "loss: 0.7163, acc: 0.6964\n",
      "loss: 0.7173, acc: 0.6976\n",
      "loss: 0.7167, acc: 0.6992\n",
      "loss: 0.7195, acc: 0.6975\n",
      "loss: 0.7183, acc: 0.6975\n",
      "> val_acc: 0.6156, val_f1: 0.5959\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.7264, acc: 0.7344\n",
      "loss: 0.7674, acc: 0.6589\n",
      "loss: 0.7551, acc: 0.6676\n",
      "loss: 0.7240, acc: 0.6836\n",
      "loss: 0.7186, acc: 0.6905\n",
      "loss: 0.7097, acc: 0.6983\n",
      "loss: 0.7160, acc: 0.7001\n",
      "loss: 0.7224, acc: 0.6953\n",
      "loss: 0.7218, acc: 0.6970\n",
      "loss: 0.7197, acc: 0.6990\n",
      "loss: 0.7250, acc: 0.6952\n",
      "loss: 0.7245, acc: 0.6956\n",
      "loss: 0.7221, acc: 0.6962\n",
      "loss: 0.7211, acc: 0.6960\n",
      "loss: 0.7226, acc: 0.6948\n",
      "loss: 0.7188, acc: 0.6961\n",
      "loss: 0.7206, acc: 0.6946\n",
      "loss: 0.7209, acc: 0.6939\n",
      "loss: 0.7201, acc: 0.6935\n",
      "loss: 0.7186, acc: 0.6935\n",
      "> val_acc: 0.6749, val_f1: 0.6558\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6749\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.6704, acc: 0.7344\n",
      "loss: 0.6668, acc: 0.7188\n",
      "loss: 0.7007, acc: 0.7067\n",
      "loss: 0.6956, acc: 0.7005\n",
      "loss: 0.6849, acc: 0.7079\n",
      "loss: 0.6821, acc: 0.7115\n",
      "loss: 0.6753, acc: 0.7135\n",
      "loss: 0.6799, acc: 0.7122\n",
      "loss: 0.6888, acc: 0.7086\n",
      "loss: 0.6841, acc: 0.7103\n",
      "loss: 0.6808, acc: 0.7111\n",
      "loss: 0.6813, acc: 0.7091\n",
      "loss: 0.6839, acc: 0.7086\n",
      "loss: 0.6849, acc: 0.7096\n",
      "loss: 0.6878, acc: 0.7087\n",
      "loss: 0.6891, acc: 0.7083\n",
      "loss: 0.6945, acc: 0.7048\n",
      "loss: 0.6919, acc: 0.7058\n",
      "loss: 0.6964, acc: 0.7046\n",
      "loss: 0.7008, acc: 0.7018\n",
      "> val_acc: 0.6777, val_f1: 0.6576\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6777\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.6435, acc: 0.7406\n",
      "loss: 0.6622, acc: 0.7422\n",
      "loss: 0.6819, acc: 0.7208\n",
      "loss: 0.6909, acc: 0.7148\n",
      "loss: 0.6934, acc: 0.7106\n",
      "loss: 0.6865, acc: 0.7120\n",
      "loss: 0.6823, acc: 0.7138\n",
      "loss: 0.6841, acc: 0.7133\n",
      "loss: 0.6895, acc: 0.7111\n",
      "loss: 0.6840, acc: 0.7137\n",
      "loss: 0.6815, acc: 0.7173\n",
      "loss: 0.6855, acc: 0.7141\n",
      "loss: 0.6889, acc: 0.7106\n",
      "loss: 0.6921, acc: 0.7103\n",
      "loss: 0.6906, acc: 0.7113\n",
      "loss: 0.6946, acc: 0.7068\n",
      "loss: 0.6949, acc: 0.7081\n",
      "loss: 0.6967, acc: 0.7075\n",
      "loss: 0.6969, acc: 0.7077\n",
      "> val_acc: 0.6720, val_f1: 0.6536\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.5701, acc: 0.8047\n",
      "loss: 0.6690, acc: 0.7321\n",
      "loss: 0.7014, acc: 0.7122\n",
      "loss: 0.6760, acc: 0.7178\n",
      "loss: 0.6738, acc: 0.7195\n",
      "loss: 0.6846, acc: 0.7159\n",
      "loss: 0.6840, acc: 0.7109\n",
      "loss: 0.6814, acc: 0.7128\n",
      "loss: 0.6815, acc: 0.7124\n",
      "loss: 0.6884, acc: 0.7078\n",
      "loss: 0.6891, acc: 0.7058\n",
      "loss: 0.6958, acc: 0.7039\n",
      "loss: 0.6919, acc: 0.7069\n",
      "loss: 0.6926, acc: 0.7057\n",
      "loss: 0.6969, acc: 0.7036\n",
      "loss: 0.6977, acc: 0.7015\n",
      "loss: 0.6969, acc: 0.7037\n",
      "loss: 0.6917, acc: 0.7062\n",
      "loss: 0.6923, acc: 0.7065\n",
      "loss: 0.6933, acc: 0.7065\n",
      "> val_acc: 0.6676, val_f1: 0.6270\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.7402, acc: 0.6875\n",
      "loss: 0.7136, acc: 0.6962\n",
      "loss: 0.6936, acc: 0.6987\n",
      "loss: 0.6923, acc: 0.7113\n",
      "loss: 0.6867, acc: 0.7083\n",
      "loss: 0.6850, acc: 0.7074\n",
      "loss: 0.6870, acc: 0.7105\n",
      "loss: 0.6794, acc: 0.7147\n",
      "loss: 0.6856, acc: 0.7134\n",
      "loss: 0.6837, acc: 0.7159\n",
      "loss: 0.6790, acc: 0.7182\n",
      "loss: 0.6812, acc: 0.7182\n",
      "loss: 0.6853, acc: 0.7183\n",
      "loss: 0.6857, acc: 0.7185\n",
      "loss: 0.6881, acc: 0.7145\n",
      "loss: 0.6898, acc: 0.7130\n",
      "loss: 0.6882, acc: 0.7147\n",
      "loss: 0.6907, acc: 0.7149\n",
      "loss: 0.6898, acc: 0.7149\n",
      "> val_acc: 0.6720, val_f1: 0.6547\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.8016, acc: 0.7188\n",
      "loss: 0.6799, acc: 0.7526\n",
      "loss: 0.6596, acc: 0.7514\n",
      "loss: 0.6552, acc: 0.7451\n",
      "loss: 0.6633, acc: 0.7366\n",
      "loss: 0.6626, acc: 0.7392\n",
      "loss: 0.6735, acc: 0.7308\n",
      "loss: 0.6762, acc: 0.7279\n",
      "loss: 0.6770, acc: 0.7245\n",
      "loss: 0.6716, acc: 0.7255\n",
      "loss: 0.6674, acc: 0.7249\n",
      "loss: 0.6752, acc: 0.7213\n",
      "loss: 0.6792, acc: 0.7198\n",
      "loss: 0.6804, acc: 0.7183\n",
      "loss: 0.6773, acc: 0.7172\n",
      "loss: 0.6729, acc: 0.7214\n",
      "loss: 0.6735, acc: 0.7191\n",
      "loss: 0.6778, acc: 0.7160\n",
      "loss: 0.6777, acc: 0.7163\n",
      "loss: 0.6820, acc: 0.7135\n",
      "> val_acc: 0.6561, val_f1: 0.6497\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.6411, acc: 0.7240\n",
      "loss: 0.6198, acc: 0.7598\n",
      "loss: 0.6484, acc: 0.7416\n",
      "loss: 0.6566, acc: 0.7248\n",
      "loss: 0.6606, acc: 0.7283\n",
      "loss: 0.6802, acc: 0.7165\n",
      "loss: 0.6804, acc: 0.7159\n",
      "loss: 0.6750, acc: 0.7192\n",
      "loss: 0.6736, acc: 0.7188\n",
      "loss: 0.6760, acc: 0.7184\n",
      "loss: 0.6714, acc: 0.7220\n",
      "loss: 0.6739, acc: 0.7185\n",
      "loss: 0.6754, acc: 0.7173\n",
      "loss: 0.6750, acc: 0.7185\n",
      "loss: 0.6788, acc: 0.7166\n",
      "loss: 0.6778, acc: 0.7188\n",
      "loss: 0.6778, acc: 0.7180\n",
      "loss: 0.6806, acc: 0.7157\n",
      "loss: 0.6826, acc: 0.7144\n",
      "loss: 0.6795, acc: 0.7165\n",
      "> val_acc: 0.6734, val_f1: 0.6488\n",
      ">> test_acc: 0.6777, test_f1: 0.6576\n"
     ]
    }
   ],
   "source": [
    "model_name = 'lstm'\n",
    "dataset = 'twitter' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_lstm = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_lstm)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGAN - laptop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: laptop_tokenizer.dat\n",
      "loading embedding_matrix: 300_laptop_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.MGAN'>\n",
      "> dataset_file: {'train': './datasets/semeval14/Laptops_Train.xml.seg', 'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'}\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a18039ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: mgan\n",
      "> dataset: laptop\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.0530, acc: 0.4125\n",
      "loss: 1.0532, acc: 0.4266\n",
      "loss: 1.0600, acc: 0.4229\n",
      "loss: 1.0551, acc: 0.4414\n",
      "loss: 1.0535, acc: 0.4412\n",
      "loss: 1.0473, acc: 0.4490\n",
      "loss: 1.0435, acc: 0.4576\n",
      "> val_acc: 0.5439, val_f1: 0.3636\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.5439\n",
      "                                                                                                    \n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0009, acc: 0.5729\n",
      "loss: 0.9812, acc: 0.5547\n",
      "loss: 0.9715, acc: 0.5625\n",
      "loss: 0.9562, acc: 0.5773\n",
      "loss: 0.9310, acc: 0.5944\n",
      "loss: 0.9242, acc: 0.5926\n",
      "loss: 0.9167, acc: 0.5919\n",
      "> val_acc: 0.6066, val_f1: 0.4426\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.6066\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.7719, acc: 0.6719\n",
      "loss: 0.7678, acc: 0.6719\n",
      "loss: 0.7814, acc: 0.6619\n",
      "loss: 0.7491, acc: 0.6826\n",
      "loss: 0.7457, acc: 0.6838\n",
      "loss: 0.7356, acc: 0.6887\n",
      "loss: 0.7405, acc: 0.6875\n",
      "loss: 0.7490, acc: 0.6801\n",
      "> val_acc: 0.6536, val_f1: 0.5308\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.6536\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.7129, acc: 0.6641\n",
      "loss: 0.6879, acc: 0.7031\n",
      "loss: 0.7064, acc: 0.6964\n",
      "loss: 0.6876, acc: 0.7072\n",
      "loss: 0.6949, acc: 0.7077\n",
      "loss: 0.6899, acc: 0.7117\n",
      "loss: 0.6977, acc: 0.7063\n",
      "> val_acc: 0.6630, val_f1: 0.5363\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.663\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.7578, acc: 0.6484\n",
      "loss: 0.6399, acc: 0.7411\n",
      "loss: 0.6377, acc: 0.7487\n",
      "loss: 0.6295, acc: 0.7528\n",
      "loss: 0.6449, acc: 0.7443\n",
      "loss: 0.6501, acc: 0.7402\n",
      "loss: 0.6507, acc: 0.7407\n",
      "loss: 0.6617, acc: 0.7332\n",
      "> val_acc: 0.6818, val_f1: 0.6181\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.6818\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.7134, acc: 0.6719\n",
      "loss: 0.6981, acc: 0.7047\n",
      "loss: 0.7109, acc: 0.6958\n",
      "loss: 0.7047, acc: 0.7031\n",
      "loss: 0.7078, acc: 0.7031\n",
      "loss: 0.7166, acc: 0.6979\n",
      "loss: 0.7249, acc: 0.6933\n",
      "> val_acc: 0.6332, val_f1: 0.5389\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.6728, acc: 0.7240\n",
      "loss: 0.6595, acc: 0.7305\n",
      "loss: 0.6401, acc: 0.7368\n",
      "loss: 0.6225, acc: 0.7500\n",
      "loss: 0.6419, acc: 0.7385\n",
      "loss: 0.6381, acc: 0.7444\n",
      "loss: 0.6499, acc: 0.7348\n",
      "> val_acc: 0.6646, val_f1: 0.6024\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.7232, acc: 0.7344\n",
      "loss: 0.6642, acc: 0.7109\n",
      "loss: 0.6489, acc: 0.7244\n",
      "loss: 0.6527, acc: 0.7266\n",
      "loss: 0.6452, acc: 0.7329\n",
      "loss: 0.6512, acc: 0.7326\n",
      "loss: 0.6595, acc: 0.7283\n",
      "loss: 0.6626, acc: 0.7296\n",
      "> val_acc: 0.6677, val_f1: 0.5498\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.5583, acc: 0.7969\n",
      "loss: 0.5977, acc: 0.7639\n",
      "loss: 0.6097, acc: 0.7511\n",
      "loss: 0.6108, acc: 0.7541\n",
      "loss: 0.6239, acc: 0.7507\n",
      "loss: 0.6307, acc: 0.7484\n",
      "loss: 0.6182, acc: 0.7532\n",
      "> val_acc: 0.7006, val_f1: 0.6500\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.7006\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6457, acc: 0.7734\n",
      "loss: 0.5834, acc: 0.7902\n",
      "loss: 0.5862, acc: 0.7799\n",
      "loss: 0.6034, acc: 0.7619\n",
      "loss: 0.6087, acc: 0.7564\n",
      "loss: 0.6020, acc: 0.7639\n",
      "loss: 0.6054, acc: 0.7603\n",
      "loss: 0.6091, acc: 0.7560\n",
      "> val_acc: 0.6740, val_f1: 0.5879\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.6870, acc: 0.7031\n",
      "loss: 0.6406, acc: 0.7359\n",
      "loss: 0.6517, acc: 0.7219\n",
      "loss: 0.6580, acc: 0.7234\n",
      "loss: 0.6492, acc: 0.7300\n",
      "loss: 0.6356, acc: 0.7391\n",
      "loss: 0.6292, acc: 0.7455\n",
      "> val_acc: 0.6489, val_f1: 0.5562\n",
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.6749, acc: 0.7396\n",
      "loss: 0.6372, acc: 0.7441\n",
      "loss: 0.6483, acc: 0.7356\n",
      "loss: 0.6429, acc: 0.7439\n",
      "loss: 0.6333, acc: 0.7493\n",
      "loss: 0.6195, acc: 0.7533\n",
      "loss: 0.6072, acc: 0.7599\n",
      "> val_acc: 0.6787, val_f1: 0.5941\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.4362, acc: 0.9062\n",
      "loss: 0.5321, acc: 0.7969\n",
      "loss: 0.5536, acc: 0.7841\n",
      "loss: 0.5772, acc: 0.7705\n",
      "loss: 0.5834, acc: 0.7679\n",
      "loss: 0.5741, acc: 0.7746\n",
      "loss: 0.5781, acc: 0.7712\n",
      "loss: 0.5802, acc: 0.7700\n",
      "> val_acc: 0.7069, val_f1: 0.6438\n",
      ">> saved: state_dict/mgan_laptop_val_acc0.7069\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.5134, acc: 0.8086\n",
      "loss: 0.5276, acc: 0.8003\n",
      "loss: 0.5194, acc: 0.8025\n",
      "loss: 0.5301, acc: 0.7952\n",
      "loss: 0.5403, acc: 0.7891\n",
      "loss: 0.5391, acc: 0.7953\n",
      "loss: 0.5469, acc: 0.7900\n",
      "> val_acc: 0.7006, val_f1: 0.6463\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.5791, acc: 0.7500\n",
      "loss: 0.5748, acc: 0.7589\n",
      "loss: 0.5910, acc: 0.7474\n",
      "loss: 0.5911, acc: 0.7518\n",
      "loss: 0.5851, acc: 0.7557\n",
      "loss: 0.5848, acc: 0.7541\n",
      "loss: 0.5750, acc: 0.7612\n",
      "loss: 0.5719, acc: 0.7642\n",
      "> val_acc: 0.6693, val_f1: 0.5779\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.5860, acc: 0.7812\n",
      "loss: 0.5618, acc: 0.7875\n",
      "loss: 0.5323, acc: 0.7990\n",
      "loss: 0.5427, acc: 0.7961\n",
      "loss: 0.5415, acc: 0.7969\n",
      "loss: 0.5517, acc: 0.7932\n",
      "loss: 0.5417, acc: 0.8004\n",
      "> val_acc: 0.6991, val_f1: 0.6342\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.4584, acc: 0.8750\n",
      "loss: 0.5203, acc: 0.8223\n",
      "loss: 0.5066, acc: 0.8065\n",
      "loss: 0.5102, acc: 0.8038\n",
      "loss: 0.5113, acc: 0.7996\n",
      "loss: 0.5209, acc: 0.7969\n",
      "loss: 0.5183, acc: 0.8007\n",
      "> val_acc: 0.6771, val_f1: 0.6047\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.5344, acc: 0.7500\n",
      "loss: 0.5005, acc: 0.7943\n",
      "loss: 0.4918, acc: 0.8111\n",
      "loss: 0.4900, acc: 0.8066\n",
      "loss: 0.4990, acc: 0.8051\n",
      "loss: 0.5190, acc: 0.7951\n",
      "loss: 0.5330, acc: 0.7863\n",
      "loss: 0.5353, acc: 0.7834\n",
      "> val_acc: 0.6771, val_f1: 0.5956\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.4837, acc: 0.8438\n",
      "loss: 0.5072, acc: 0.8212\n",
      "loss: 0.4885, acc: 0.8304\n",
      "loss: 0.4901, acc: 0.8306\n",
      "loss: 0.5021, acc: 0.8210\n",
      "loss: 0.5105, acc: 0.8130\n",
      "loss: 0.5204, acc: 0.8056\n",
      "> val_acc: 0.6693, val_f1: 0.6123\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.4571, acc: 0.7969\n",
      "loss: 0.4903, acc: 0.8147\n",
      "loss: 0.4845, acc: 0.8138\n",
      "loss: 0.4901, acc: 0.8143\n",
      "loss: 0.4864, acc: 0.8161\n",
      "loss: 0.4756, acc: 0.8212\n",
      "loss: 0.4869, acc: 0.8174\n",
      "loss: 0.4895, acc: 0.8157\n",
      "> val_acc: 0.6724, val_f1: 0.5849\n",
      ">> test_acc: 0.7069, test_f1: 0.6438\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mgan'\n",
    "dataset = 'laptop' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_mgan = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_mgan)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGAN - restaurant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: restaurant_tokenizer.dat\n",
      "loading embedding_matrix: 300_restaurant_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.MGAN'>\n",
      "> dataset_file: {'train': './datasets/semeval14/Restaurants_Train.xml.seg', 'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'}\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a18039ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: mgan\n",
      "> dataset: restaurant\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.0925, acc: 0.3688\n",
      "loss: 1.0113, acc: 0.5000\n",
      "loss: 0.9940, acc: 0.5417\n",
      "loss: 0.9912, acc: 0.5508\n",
      "loss: 0.9720, acc: 0.5675\n",
      "loss: 0.9661, acc: 0.5693\n",
      "loss: 0.9556, acc: 0.5728\n",
      "loss: 0.9419, acc: 0.5793\n",
      "loss: 0.9326, acc: 0.5795\n",
      "loss: 0.9139, acc: 0.5906\n",
      "loss: 0.9032, acc: 0.5935\n",
      "> val_acc: 0.7107, val_f1: 0.4740\n",
      ">> saved: state_dict/mgan_restaurant_val_acc0.7107\n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "loss: 0.7059, acc: 0.7188\n",
      "loss: 0.7414, acc: 0.6914\n",
      "loss: 0.7434, acc: 0.6899\n",
      "loss: 0.7259, acc: 0.6979\n",
      "loss: 0.7315, acc: 0.6963\n",
      "loss: 0.7415, acc: 0.6881\n",
      "loss: 0.7343, acc: 0.6913\n",
      "loss: 0.7271, acc: 0.6945\n",
      "loss: 0.7291, acc: 0.6930\n",
      "loss: 0.7278, acc: 0.6937\n",
      "loss: 0.7286, acc: 0.6919\n",
      "> val_acc: 0.7366, val_f1: 0.5236\n",
      ">> saved: state_dict/mgan_restaurant_val_acc0.7366\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.7653, acc: 0.6875\n",
      "loss: 0.7018, acc: 0.7161\n",
      "loss: 0.6880, acc: 0.6989\n",
      "loss: 0.7017, acc: 0.6826\n",
      "loss: 0.7025, acc: 0.6860\n",
      "loss: 0.7001, acc: 0.6875\n",
      "loss: 0.7027, acc: 0.6845\n",
      "loss: 0.7050, acc: 0.6862\n",
      "loss: 0.6995, acc: 0.6944\n",
      "loss: 0.7000, acc: 0.6960\n",
      "loss: 0.7027, acc: 0.6942\n",
      "loss: 0.7016, acc: 0.6953\n",
      "> val_acc: 0.7438, val_f1: 0.5602\n",
      ">> saved: state_dict/mgan_restaurant_val_acc0.7438\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.7457, acc: 0.6523\n",
      "loss: 0.7101, acc: 0.6788\n",
      "loss: 0.6955, acc: 0.6819\n",
      "loss: 0.6884, acc: 0.6949\n",
      "loss: 0.6838, acc: 0.7005\n",
      "loss: 0.6841, acc: 0.7020\n",
      "loss: 0.6831, acc: 0.7077\n",
      "loss: 0.6780, acc: 0.7103\n",
      "loss: 0.6740, acc: 0.7134\n",
      "loss: 0.6768, acc: 0.7108\n",
      "loss: 0.6759, acc: 0.7133\n",
      "> val_acc: 0.7339, val_f1: 0.5200\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.6304, acc: 0.7109\n",
      "loss: 0.6338, acc: 0.7344\n",
      "loss: 0.6498, acc: 0.7357\n",
      "loss: 0.6501, acc: 0.7298\n",
      "loss: 0.6410, acc: 0.7330\n",
      "loss: 0.6425, acc: 0.7292\n",
      "loss: 0.6374, acc: 0.7280\n",
      "loss: 0.6444, acc: 0.7276\n",
      "loss: 0.6439, acc: 0.7292\n",
      "loss: 0.6613, acc: 0.7251\n",
      "loss: 0.6700, acc: 0.7209\n",
      "loss: 0.6688, acc: 0.7217\n",
      "> val_acc: 0.7536, val_f1: 0.5556\n",
      ">> saved: state_dict/mgan_restaurant_val_acc0.7536\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.6880, acc: 0.7125\n",
      "loss: 0.6602, acc: 0.7203\n",
      "loss: 0.6436, acc: 0.7354\n",
      "loss: 0.6550, acc: 0.7281\n",
      "loss: 0.6497, acc: 0.7306\n",
      "loss: 0.6399, acc: 0.7354\n",
      "loss: 0.6422, acc: 0.7375\n",
      "loss: 0.6476, acc: 0.7316\n",
      "loss: 0.6521, acc: 0.7285\n",
      "loss: 0.6485, acc: 0.7300\n",
      "loss: 0.6485, acc: 0.7327\n",
      "> val_acc: 0.7330, val_f1: 0.5261\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.6639, acc: 0.7031\n",
      "loss: 0.6626, acc: 0.7188\n",
      "loss: 0.6756, acc: 0.7067\n",
      "loss: 0.6539, acc: 0.7144\n",
      "loss: 0.6504, acc: 0.7235\n",
      "loss: 0.6568, acc: 0.7204\n",
      "loss: 0.6504, acc: 0.7263\n",
      "loss: 0.6467, acc: 0.7311\n",
      "loss: 0.6481, acc: 0.7322\n",
      "loss: 0.6422, acc: 0.7347\n",
      "loss: 0.6410, acc: 0.7326\n",
      "> val_acc: 0.7634, val_f1: 0.6254\n",
      ">> saved: state_dict/mgan_restaurant_val_acc0.7634\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.5752, acc: 0.7344\n",
      "loss: 0.6239, acc: 0.7448\n",
      "loss: 0.6314, acc: 0.7330\n",
      "loss: 0.6199, acc: 0.7305\n",
      "loss: 0.6136, acc: 0.7359\n",
      "loss: 0.6095, acc: 0.7428\n",
      "loss: 0.6064, acc: 0.7455\n",
      "loss: 0.6156, acc: 0.7418\n",
      "loss: 0.6133, acc: 0.7458\n",
      "loss: 0.6146, acc: 0.7456\n",
      "loss: 0.6207, acc: 0.7436\n",
      "loss: 0.6196, acc: 0.7427\n",
      "> val_acc: 0.7473, val_f1: 0.5677\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.6148, acc: 0.7734\n",
      "loss: 0.5998, acc: 0.7691\n",
      "loss: 0.6174, acc: 0.7511\n",
      "loss: 0.6098, acc: 0.7582\n",
      "loss: 0.6217, acc: 0.7552\n",
      "loss: 0.6190, acc: 0.7565\n",
      "loss: 0.6142, acc: 0.7541\n",
      "loss: 0.6159, acc: 0.7520\n",
      "loss: 0.6199, acc: 0.7479\n",
      "loss: 0.6261, acc: 0.7443\n",
      "loss: 0.6327, acc: 0.7407\n",
      "> val_acc: 0.7634, val_f1: 0.6096\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6819, acc: 0.7500\n",
      "loss: 0.6021, acc: 0.7634\n",
      "loss: 0.6033, acc: 0.7630\n",
      "loss: 0.6007, acc: 0.7610\n",
      "loss: 0.6005, acc: 0.7550\n",
      "loss: 0.6081, acc: 0.7535\n",
      "loss: 0.6113, acc: 0.7505\n",
      "loss: 0.6103, acc: 0.7508\n",
      "loss: 0.6036, acc: 0.7545\n",
      "loss: 0.6013, acc: 0.7533\n",
      "loss: 0.6033, acc: 0.7524\n",
      "loss: 0.6044, acc: 0.7503\n",
      "> val_acc: 0.7616, val_f1: 0.6290\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.5351, acc: 0.8063\n",
      "loss: 0.5588, acc: 0.7953\n",
      "loss: 0.5716, acc: 0.7802\n",
      "loss: 0.5988, acc: 0.7672\n",
      "loss: 0.6001, acc: 0.7612\n",
      "loss: 0.6021, acc: 0.7589\n",
      "loss: 0.5995, acc: 0.7603\n",
      "loss: 0.5967, acc: 0.7617\n",
      "loss: 0.5958, acc: 0.7615\n",
      "loss: 0.5979, acc: 0.7591\n",
      "loss: 0.5990, acc: 0.7582\n",
      "> val_acc: 0.7804, val_f1: 0.6686\n",
      ">> saved: state_dict/mgan_restaurant_val_acc0.7804\n",
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.5371, acc: 0.7812\n",
      "loss: 0.5820, acc: 0.7559\n",
      "loss: 0.6153, acc: 0.7392\n",
      "loss: 0.6071, acc: 0.7422\n",
      "loss: 0.6059, acc: 0.7446\n",
      "loss: 0.6060, acc: 0.7467\n",
      "loss: 0.6008, acc: 0.7552\n",
      "loss: 0.6033, acc: 0.7537\n",
      "loss: 0.5939, acc: 0.7558\n",
      "loss: 0.5933, acc: 0.7555\n",
      "loss: 0.5903, acc: 0.7565\n",
      "> val_acc: 0.7571, val_f1: 0.6017\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.5228, acc: 0.8125\n",
      "loss: 0.5822, acc: 0.7865\n",
      "loss: 0.6195, acc: 0.7670\n",
      "loss: 0.6159, acc: 0.7578\n",
      "loss: 0.6013, acc: 0.7649\n",
      "loss: 0.5897, acc: 0.7656\n",
      "loss: 0.5890, acc: 0.7651\n",
      "loss: 0.5813, acc: 0.7691\n",
      "loss: 0.5813, acc: 0.7713\n",
      "loss: 0.5841, acc: 0.7707\n",
      "loss: 0.5860, acc: 0.7675\n",
      "loss: 0.5865, acc: 0.7679\n",
      "> val_acc: 0.7500, val_f1: 0.5633\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.6001, acc: 0.7461\n",
      "loss: 0.5704, acc: 0.7674\n",
      "loss: 0.5820, acc: 0.7623\n",
      "loss: 0.5734, acc: 0.7656\n",
      "loss: 0.5557, acc: 0.7773\n",
      "loss: 0.5602, acc: 0.7759\n",
      "loss: 0.5656, acc: 0.7698\n",
      "loss: 0.5647, acc: 0.7696\n",
      "loss: 0.5680, acc: 0.7663\n",
      "loss: 0.5762, acc: 0.7621\n",
      "loss: 0.5788, acc: 0.7613\n",
      "> val_acc: 0.7616, val_f1: 0.6083\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.5929, acc: 0.7422\n",
      "loss: 0.5949, acc: 0.7790\n",
      "loss: 0.5900, acc: 0.7656\n",
      "loss: 0.5864, acc: 0.7693\n",
      "loss: 0.5825, acc: 0.7635\n",
      "loss: 0.5750, acc: 0.7645\n",
      "loss: 0.5701, acc: 0.7656\n",
      "loss: 0.5738, acc: 0.7610\n",
      "loss: 0.5731, acc: 0.7597\n",
      "loss: 0.5755, acc: 0.7576\n",
      "loss: 0.5768, acc: 0.7575\n",
      "loss: 0.5724, acc: 0.7603\n",
      "> val_acc: 0.7250, val_f1: 0.5483\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.5800, acc: 0.7688\n",
      "loss: 0.5685, acc: 0.7766\n",
      "loss: 0.5764, acc: 0.7740\n",
      "loss: 0.5884, acc: 0.7680\n",
      "loss: 0.5886, acc: 0.7675\n",
      "loss: 0.5803, acc: 0.7677\n",
      "loss: 0.5687, acc: 0.7705\n",
      "loss: 0.5683, acc: 0.7691\n",
      "loss: 0.5693, acc: 0.7674\n",
      "loss: 0.5691, acc: 0.7659\n",
      "loss: 0.5676, acc: 0.7653\n",
      "> val_acc: 0.7795, val_f1: 0.6628\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.5777, acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5421, acc: 0.7891\n",
      "loss: 0.5601, acc: 0.7776\n",
      "loss: 0.5712, acc: 0.7778\n",
      "loss: 0.5583, acc: 0.7779\n",
      "loss: 0.5629, acc: 0.7734\n",
      "loss: 0.5628, acc: 0.7670\n",
      "loss: 0.5656, acc: 0.7681\n",
      "loss: 0.5636, acc: 0.7703\n",
      "loss: 0.5610, acc: 0.7708\n",
      "loss: 0.5586, acc: 0.7712\n",
      "> val_acc: 0.7518, val_f1: 0.5932\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.6011, acc: 0.7500\n",
      "loss: 0.5158, acc: 0.8047\n",
      "loss: 0.5100, acc: 0.8040\n",
      "loss: 0.5116, acc: 0.8066\n",
      "loss: 0.5211, acc: 0.7969\n",
      "loss: 0.5372, acc: 0.7903\n",
      "loss: 0.5402, acc: 0.7878\n",
      "loss: 0.5390, acc: 0.7891\n",
      "loss: 0.5395, acc: 0.7862\n",
      "loss: 0.5405, acc: 0.7829\n",
      "loss: 0.5431, acc: 0.7819\n",
      "loss: 0.5500, acc: 0.7799\n",
      "> val_acc: 0.7696, val_f1: 0.6512\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.5196, acc: 0.7695\n",
      "loss: 0.5191, acc: 0.7778\n",
      "loss: 0.5310, acc: 0.7723\n",
      "loss: 0.5287, acc: 0.7755\n",
      "loss: 0.5358, acc: 0.7754\n",
      "loss: 0.5383, acc: 0.7780\n",
      "loss: 0.5357, acc: 0.7812\n",
      "loss: 0.5362, acc: 0.7833\n",
      "loss: 0.5359, acc: 0.7823\n",
      "loss: 0.5346, acc: 0.7857\n",
      "loss: 0.5344, acc: 0.7844\n",
      "> val_acc: 0.7705, val_f1: 0.6477\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.4707, acc: 0.8203\n",
      "loss: 0.5585, acc: 0.7612\n",
      "loss: 0.5224, acc: 0.7995\n",
      "loss: 0.5291, acc: 0.7923\n",
      "loss: 0.5167, acc: 0.7962\n",
      "loss: 0.5258, acc: 0.7899\n",
      "loss: 0.5143, acc: 0.7949\n",
      "loss: 0.5199, acc: 0.7956\n",
      "loss: 0.5270, acc: 0.7902\n",
      "loss: 0.5294, acc: 0.7889\n",
      "loss: 0.5277, acc: 0.7900\n",
      "loss: 0.5303, acc: 0.7891\n",
      "> val_acc: 0.7679, val_f1: 0.6597\n",
      ">> test_acc: 0.7804, test_f1: 0.6686\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mgan'\n",
    "dataset = 'restaurant' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_mgan = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_mgan)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGAN - twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: twitter_tokenizer.dat\n",
      "loading embedding_matrix: 300_twitter_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.MGAN'>\n",
      "> dataset_file: {'train': './datasets/acl-14-short-data/train.raw', 'test': './datasets/acl-14-short-data/test.raw'}\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a18039ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: mgan\n",
      "> dataset: twitter\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.0381, acc: 0.4813\n",
      "loss: 1.0391, acc: 0.4594\n",
      "loss: 1.0097, acc: 0.4833\n",
      "loss: 1.0068, acc: 0.4891\n",
      "loss: 0.9899, acc: 0.5125\n",
      "loss: 0.9928, acc: 0.5109\n",
      "loss: 0.9934, acc: 0.5147\n",
      "loss: 0.9879, acc: 0.5246\n",
      "loss: 0.9839, acc: 0.5240\n",
      "loss: 0.9817, acc: 0.5241\n",
      "loss: 0.9775, acc: 0.5293\n",
      "loss: 0.9745, acc: 0.5323\n",
      "loss: 0.9758, acc: 0.5327\n",
      "loss: 0.9745, acc: 0.5317\n",
      "loss: 0.9747, acc: 0.5306\n",
      "loss: 0.9742, acc: 0.5316\n",
      "loss: 0.9728, acc: 0.5327\n",
      "loss: 0.9706, acc: 0.5347\n",
      "loss: 0.9680, acc: 0.5365\n",
      "> val_acc: 0.5592, val_f1: 0.4508\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.5592\n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "loss: 0.9328, acc: 0.5469\n",
      "loss: 0.9503, acc: 0.5536\n",
      "loss: 0.9424, acc: 0.5612\n",
      "loss: 0.9436, acc: 0.5570\n",
      "loss: 0.9513, acc: 0.5462\n",
      "loss: 0.9393, acc: 0.5532\n",
      "loss: 0.9390, acc: 0.5532\n",
      "loss: 0.9294, acc: 0.5591\n",
      "loss: 0.9252, acc: 0.5614\n",
      "loss: 0.9270, acc: 0.5605\n",
      "loss: 0.9249, acc: 0.5631\n",
      "loss: 0.9275, acc: 0.5620\n",
      "loss: 0.9267, acc: 0.5633\n",
      "loss: 0.9265, acc: 0.5648\n",
      "loss: 0.9257, acc: 0.5645\n",
      "loss: 0.9258, acc: 0.5649\n",
      "loss: 0.9227, acc: 0.5667\n",
      "loss: 0.9196, acc: 0.5702\n",
      "loss: 0.9197, acc: 0.5708\n",
      "loss: 0.9160, acc: 0.5738\n",
      "> val_acc: 0.6156, val_f1: 0.5328\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.6156\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.8112, acc: 0.6484\n",
      "loss: 0.8134, acc: 0.6372\n",
      "loss: 0.8211, acc: 0.6384\n",
      "loss: 0.8301, acc: 0.6365\n",
      "loss: 0.8333, acc: 0.6237\n",
      "loss: 0.8424, acc: 0.6115\n",
      "loss: 0.8429, acc: 0.6176\n",
      "loss: 0.8487, acc: 0.6130\n",
      "loss: 0.8411, acc: 0.6172\n",
      "loss: 0.8329, acc: 0.6253\n",
      "loss: 0.8242, acc: 0.6293\n",
      "loss: 0.8235, acc: 0.6298\n",
      "loss: 0.8226, acc: 0.6287\n",
      "loss: 0.8219, acc: 0.6286\n",
      "loss: 0.8190, acc: 0.6296\n",
      "loss: 0.8159, acc: 0.6303\n",
      "loss: 0.8131, acc: 0.6334\n",
      "loss: 0.8108, acc: 0.6359\n",
      "loss: 0.8080, acc: 0.6361\n",
      "> val_acc: 0.6488, val_f1: 0.6269\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.6488\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.8621, acc: 0.6094\n",
      "loss: 0.7300, acc: 0.7188\n",
      "loss: 0.7370, acc: 0.6832\n",
      "loss: 0.7334, acc: 0.6914\n",
      "loss: 0.7328, acc: 0.6890\n",
      "loss: 0.7330, acc: 0.6851\n",
      "loss: 0.7520, acc: 0.6724\n",
      "loss: 0.7617, acc: 0.6680\n",
      "loss: 0.7661, acc: 0.6646\n",
      "loss: 0.7668, acc: 0.6641\n",
      "loss: 0.7647, acc: 0.6679\n",
      "loss: 0.7695, acc: 0.6663\n",
      "loss: 0.7718, acc: 0.6644\n",
      "loss: 0.7696, acc: 0.6650\n",
      "loss: 0.7679, acc: 0.6675\n",
      "loss: 0.7682, acc: 0.6700\n",
      "loss: 0.7674, acc: 0.6713\n",
      "loss: 0.7657, acc: 0.6695\n",
      "loss: 0.7616, acc: 0.6702\n",
      "loss: 0.7618, acc: 0.6691\n",
      "> val_acc: 0.6402, val_f1: 0.5764\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.7914, acc: 0.6354\n",
      "loss: 0.7149, acc: 0.6973\n",
      "loss: 0.7025, acc: 0.7067\n",
      "loss: 0.7166, acc: 0.6979\n",
      "loss: 0.7202, acc: 0.6936\n",
      "loss: 0.7194, acc: 0.6931\n",
      "loss: 0.7210, acc: 0.6913\n",
      "loss: 0.7171, acc: 0.6998\n",
      "loss: 0.7145, acc: 0.6995\n",
      "loss: 0.7130, acc: 0.7008\n",
      "loss: 0.7085, acc: 0.7011\n",
      "loss: 0.7176, acc: 0.6953\n",
      "loss: 0.7282, acc: 0.6890\n",
      "loss: 0.7297, acc: 0.6886\n",
      "loss: 0.7275, acc: 0.6901\n",
      "loss: 0.7274, acc: 0.6899\n",
      "loss: 0.7264, acc: 0.6899\n",
      "loss: 0.7305, acc: 0.6871\n",
      "loss: 0.7333, acc: 0.6862\n",
      "loss: 0.7357, acc: 0.6837\n",
      "> val_acc: 0.6662, val_f1: 0.6352\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.6662\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.7208, acc: 0.7281\n",
      "loss: 0.7131, acc: 0.7203\n",
      "loss: 0.7216, acc: 0.7042\n",
      "loss: 0.6968, acc: 0.7234\n",
      "loss: 0.7060, acc: 0.7163\n",
      "loss: 0.6992, acc: 0.7193\n",
      "loss: 0.6986, acc: 0.7196\n",
      "loss: 0.7035, acc: 0.7148\n",
      "loss: 0.7083, acc: 0.7083\n",
      "loss: 0.7087, acc: 0.7072\n",
      "loss: 0.7097, acc: 0.7071\n",
      "loss: 0.7155, acc: 0.7010\n",
      "loss: 0.7167, acc: 0.6998\n",
      "loss: 0.7141, acc: 0.7020\n",
      "loss: 0.7126, acc: 0.7017\n",
      "loss: 0.7163, acc: 0.7008\n",
      "loss: 0.7184, acc: 0.6982\n",
      "loss: 0.7201, acc: 0.6976\n",
      "loss: 0.7200, acc: 0.6975\n",
      "> val_acc: 0.6604, val_f1: 0.6057\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.6065, acc: 0.7266\n",
      "loss: 0.6550, acc: 0.7411\n",
      "loss: 0.6465, acc: 0.7513\n",
      "loss: 0.6528, acc: 0.7445\n",
      "loss: 0.6710, acc: 0.7365\n",
      "loss: 0.6841, acc: 0.7292\n",
      "loss: 0.6878, acc: 0.7222\n",
      "loss: 0.6906, acc: 0.7154\n",
      "loss: 0.6963, acc: 0.7102\n",
      "loss: 0.7040, acc: 0.7038\n",
      "loss: 0.7089, acc: 0.6989\n",
      "loss: 0.7060, acc: 0.7018\n",
      "loss: 0.7096, acc: 0.7009\n",
      "loss: 0.7128, acc: 0.6973\n",
      "loss: 0.7162, acc: 0.6936\n",
      "loss: 0.7174, acc: 0.6912\n",
      "loss: 0.7147, acc: 0.6942\n",
      "loss: 0.7171, acc: 0.6916\n",
      "loss: 0.7159, acc: 0.6923\n",
      "loss: 0.7165, acc: 0.6928\n",
      "> val_acc: 0.6691, val_f1: 0.6344\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.6691\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.7440, acc: 0.6836\n",
      "loss: 0.7260, acc: 0.6927\n",
      "loss: 0.7346, acc: 0.6819\n",
      "loss: 0.7264, acc: 0.6941\n",
      "loss: 0.7153, acc: 0.6986\n",
      "loss: 0.7102, acc: 0.6983\n",
      "loss: 0.7068, acc: 0.6999\n",
      "loss: 0.7079, acc: 0.6999\n",
      "loss: 0.7093, acc: 0.6992\n",
      "loss: 0.7108, acc: 0.6974\n",
      "loss: 0.7092, acc: 0.6976\n",
      "loss: 0.7088, acc: 0.6992\n",
      "loss: 0.7106, acc: 0.6990\n",
      "loss: 0.7096, acc: 0.7009\n",
      "loss: 0.7063, acc: 0.7031\n",
      "loss: 0.7095, acc: 0.7008\n",
      "loss: 0.7119, acc: 0.6994\n",
      "loss: 0.7114, acc: 0.6984\n",
      "loss: 0.7097, acc: 0.7001\n",
      "> val_acc: 0.6922, val_f1: 0.6607\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.6922\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.7076, acc: 0.7344\n",
      "loss: 0.6407, acc: 0.7135\n",
      "loss: 0.6905, acc: 0.6974\n",
      "loss: 0.6839, acc: 0.7080\n",
      "loss: 0.6890, acc: 0.7039\n",
      "loss: 0.6925, acc: 0.7019\n",
      "loss: 0.6957, acc: 0.7006\n",
      "loss: 0.6967, acc: 0.6997\n",
      "loss: 0.6957, acc: 0.7005\n",
      "loss: 0.6992, acc: 0.7011\n",
      "loss: 0.6978, acc: 0.7022\n",
      "loss: 0.6994, acc: 0.7017\n",
      "loss: 0.6987, acc: 0.7026\n",
      "loss: 0.7012, acc: 0.7024\n",
      "loss: 0.7026, acc: 0.7031\n",
      "loss: 0.6992, acc: 0.7048\n",
      "loss: 0.7043, acc: 0.6998\n",
      "loss: 0.7021, acc: 0.7026\n",
      "loss: 0.7032, acc: 0.7019\n",
      "loss: 0.7050, acc: 0.7012\n",
      "> val_acc: 0.6806, val_f1: 0.6464\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6785, acc: 0.6927\n",
      "loss: 0.6695, acc: 0.7109\n",
      "loss: 0.6772, acc: 0.7067\n",
      "loss: 0.6731, acc: 0.7014\n",
      "loss: 0.6708, acc: 0.7065\n",
      "loss: 0.6775, acc: 0.7042\n",
      "loss: 0.6761, acc: 0.7145\n",
      "loss: 0.6717, acc: 0.7183\n",
      "loss: 0.6692, acc: 0.7169\n",
      "loss: 0.6727, acc: 0.7152\n",
      "loss: 0.6757, acc: 0.7146\n",
      "loss: 0.6756, acc: 0.7174\n",
      "loss: 0.6717, acc: 0.7180\n",
      "loss: 0.6726, acc: 0.7183\n",
      "loss: 0.6765, acc: 0.7164\n",
      "loss: 0.6785, acc: 0.7149\n",
      "loss: 0.6806, acc: 0.7131\n",
      "loss: 0.6844, acc: 0.7111\n",
      "loss: 0.6845, acc: 0.7114\n",
      "loss: 0.6880, acc: 0.7095\n",
      "> val_acc: 0.6749, val_f1: 0.6580\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.6242, acc: 0.7750\n",
      "loss: 0.6700, acc: 0.7422\n",
      "loss: 0.6693, acc: 0.7354\n",
      "loss: 0.6639, acc: 0.7328\n",
      "loss: 0.6668, acc: 0.7244\n",
      "loss: 0.6669, acc: 0.7240\n",
      "loss: 0.6722, acc: 0.7192\n",
      "loss: 0.6762, acc: 0.7184\n",
      "loss: 0.6823, acc: 0.7142\n",
      "loss: 0.6829, acc: 0.7113\n",
      "loss: 0.6816, acc: 0.7139\n",
      "loss: 0.6828, acc: 0.7169\n",
      "loss: 0.6853, acc: 0.7130\n",
      "loss: 0.6873, acc: 0.7103\n",
      "loss: 0.6823, acc: 0.7140\n",
      "loss: 0.6795, acc: 0.7152\n",
      "loss: 0.6816, acc: 0.7156\n",
      "loss: 0.6829, acc: 0.7151\n",
      "loss: 0.6868, acc: 0.7130\n",
      "> val_acc: 0.6908, val_f1: 0.6682\n",
      "                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\n",
      "loss: 0.6487, acc: 0.7656\n",
      "loss: 0.6975, acc: 0.7054\n",
      "loss: 0.6927, acc: 0.7148\n",
      "loss: 0.6784, acc: 0.7188\n",
      "loss: 0.6742, acc: 0.7173\n",
      "loss: 0.6766, acc: 0.7170\n",
      "loss: 0.6723, acc: 0.7202\n",
      "loss: 0.6770, acc: 0.7158\n",
      "loss: 0.6747, acc: 0.7143\n",
      "loss: 0.6735, acc: 0.7158\n",
      "loss: 0.6802, acc: 0.7142\n",
      "loss: 0.6821, acc: 0.7138\n",
      "loss: 0.6787, acc: 0.7157\n",
      "loss: 0.6806, acc: 0.7155\n",
      "loss: 0.6796, acc: 0.7161\n",
      "loss: 0.6791, acc: 0.7165\n",
      "loss: 0.6777, acc: 0.7167\n",
      "loss: 0.6779, acc: 0.7161\n",
      "loss: 0.6748, acc: 0.7171\n",
      "loss: 0.6773, acc: 0.7139\n",
      "> val_acc: 0.6777, val_f1: 0.6493\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.6487, acc: 0.7227\n",
      "loss: 0.6251, acc: 0.7483\n",
      "loss: 0.6394, acc: 0.7366\n",
      "loss: 0.6560, acc: 0.7278\n",
      "loss: 0.6558, acc: 0.7220\n",
      "loss: 0.6571, acc: 0.7198\n",
      "loss: 0.6518, acc: 0.7275\n",
      "loss: 0.6546, acc: 0.7276\n",
      "loss: 0.6541, acc: 0.7280\n",
      "loss: 0.6556, acc: 0.7267\n",
      "loss: 0.6566, acc: 0.7251\n",
      "loss: 0.6585, acc: 0.7222\n",
      "loss: 0.6555, acc: 0.7249\n",
      "loss: 0.6575, acc: 0.7228\n",
      "loss: 0.6562, acc: 0.7240\n",
      "loss: 0.6584, acc: 0.7235\n",
      "loss: 0.6616, acc: 0.7212\n",
      "loss: 0.6639, acc: 0.7210\n",
      "loss: 0.6661, acc: 0.7201\n",
      "> val_acc: 0.6806, val_f1: 0.6608\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.7361, acc: 0.7031\n",
      "loss: 0.6693, acc: 0.7214\n",
      "loss: 0.6693, acc: 0.7301\n",
      "loss: 0.6743, acc: 0.7178\n",
      "loss: 0.6674, acc: 0.7240\n",
      "loss: 0.6654, acc: 0.7284\n",
      "loss: 0.6657, acc: 0.7263\n",
      "loss: 0.6671, acc: 0.7283\n",
      "loss: 0.6670, acc: 0.7275\n",
      "loss: 0.6592, acc: 0.7306\n",
      "loss: 0.6587, acc: 0.7307\n",
      "loss: 0.6513, acc: 0.7319\n",
      "loss: 0.6528, acc: 0.7295\n",
      "loss: 0.6572, acc: 0.7268\n",
      "loss: 0.6627, acc: 0.7221\n",
      "loss: 0.6659, acc: 0.7202\n",
      "loss: 0.6664, acc: 0.7197\n",
      "loss: 0.6706, acc: 0.7169\n",
      "loss: 0.6755, acc: 0.7141\n",
      "loss: 0.6783, acc: 0.7127\n",
      "> val_acc: 0.7038, val_f1: 0.6754\n",
      ">> saved: state_dict/mgan_twitter_val_acc0.7038\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.6554, acc: 0.7344\n",
      "loss: 0.6279, acc: 0.7441\n",
      "loss: 0.6345, acc: 0.7392\n",
      "loss: 0.6493, acc: 0.7370\n",
      "loss: 0.6457, acc: 0.7371\n",
      "loss: 0.6417, acc: 0.7427\n",
      "loss: 0.6474, acc: 0.7382\n",
      "loss: 0.6507, acc: 0.7364\n",
      "loss: 0.6464, acc: 0.7355\n",
      "loss: 0.6502, acc: 0.7354\n",
      "loss: 0.6585, acc: 0.7291\n",
      "loss: 0.6639, acc: 0.7228\n",
      "loss: 0.6668, acc: 0.7195\n",
      "loss: 0.6627, acc: 0.7227\n",
      "loss: 0.6660, acc: 0.7202\n",
      "loss: 0.6630, acc: 0.7234\n",
      "loss: 0.6623, acc: 0.7235\n",
      "loss: 0.6634, acc: 0.7223\n",
      "loss: 0.6626, acc: 0.7216\n",
      "loss: 0.6654, acc: 0.7202\n",
      "> val_acc: 0.6474, val_f1: 0.5977\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.6203, acc: 0.7344\n",
      "loss: 0.6411, acc: 0.7344\n",
      "loss: 0.6485, acc: 0.7281\n",
      "loss: 0.6475, acc: 0.7430\n",
      "loss: 0.6597, acc: 0.7338\n",
      "loss: 0.6568, acc: 0.7297\n",
      "loss: 0.6502, acc: 0.7290\n",
      "loss: 0.6558, acc: 0.7254\n",
      "loss: 0.6576, acc: 0.7240\n",
      "loss: 0.6607, acc: 0.7222\n",
      "loss: 0.6639, acc: 0.7216\n",
      "loss: 0.6641, acc: 0.7195\n",
      "loss: 0.6622, acc: 0.7209\n",
      "loss: 0.6621, acc: 0.7221\n",
      "loss: 0.6628, acc: 0.7219\n",
      "loss: 0.6601, acc: 0.7236\n",
      "loss: 0.6578, acc: 0.7252\n",
      "loss: 0.6554, acc: 0.7276\n",
      "loss: 0.6571, acc: 0.7243\n",
      "> val_acc: 0.6546, val_f1: 0.6462\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.7673, acc: 0.6172\n",
      "loss: 0.6642, acc: 0.7254\n",
      "loss: 0.6646, acc: 0.7201\n",
      "loss: 0.6470, acc: 0.7298\n",
      "loss: 0.6383, acc: 0.7322\n",
      "loss: 0.6398, acc: 0.7350\n",
      "loss: 0.6346, acc: 0.7368\n",
      "loss: 0.6394, acc: 0.7314\n",
      "loss: 0.6406, acc: 0.7314\n",
      "loss: 0.6378, acc: 0.7347\n",
      "loss: 0.6393, acc: 0.7368\n",
      "loss: 0.6394, acc: 0.7360\n",
      "loss: 0.6411, acc: 0.7339\n",
      "loss: 0.6402, acc: 0.7346\n",
      "loss: 0.6418, acc: 0.7346\n",
      "loss: 0.6452, acc: 0.7332\n",
      "loss: 0.6478, acc: 0.7323\n",
      "loss: 0.6492, acc: 0.7302\n",
      "loss: 0.6497, acc: 0.7294\n",
      "loss: 0.6525, acc: 0.7273\n",
      "> val_acc: 0.6691, val_f1: 0.6529\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.5964, acc: 0.8047\n",
      "loss: 0.6375, acc: 0.7604\n",
      "loss: 0.6292, acc: 0.7589\n",
      "loss: 0.6294, acc: 0.7508\n",
      "loss: 0.6328, acc: 0.7539\n",
      "loss: 0.6283, acc: 0.7559\n",
      "loss: 0.6334, acc: 0.7495\n",
      "loss: 0.6375, acc: 0.7456\n",
      "loss: 0.6439, acc: 0.7418\n",
      "loss: 0.6445, acc: 0.7401\n",
      "loss: 0.6427, acc: 0.7390\n",
      "loss: 0.6456, acc: 0.7357\n",
      "loss: 0.6484, acc: 0.7332\n",
      "loss: 0.6480, acc: 0.7326\n",
      "loss: 0.6468, acc: 0.7308\n",
      "loss: 0.6510, acc: 0.7271\n",
      "loss: 0.6505, acc: 0.7281\n",
      "loss: 0.6520, acc: 0.7275\n",
      "loss: 0.6516, acc: 0.7281\n",
      "> val_acc: 0.6763, val_f1: 0.6362\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.7110, acc: 0.7188\n",
      "loss: 0.6473, acc: 0.7161\n",
      "loss: 0.6551, acc: 0.7230\n",
      "loss: 0.6358, acc: 0.7363\n",
      "loss: 0.6384, acc: 0.7374\n",
      "loss: 0.6368, acc: 0.7398\n",
      "loss: 0.6362, acc: 0.7389\n",
      "loss: 0.6342, acc: 0.7391\n",
      "loss: 0.6304, acc: 0.7405\n",
      "loss: 0.6295, acc: 0.7418\n",
      "loss: 0.6393, acc: 0.7381\n",
      "loss: 0.6391, acc: 0.7383\n",
      "loss: 0.6380, acc: 0.7395\n",
      "loss: 0.6431, acc: 0.7375\n",
      "loss: 0.6437, acc: 0.7368\n",
      "loss: 0.6425, acc: 0.7373\n",
      "loss: 0.6478, acc: 0.7336\n",
      "loss: 0.6489, acc: 0.7333\n",
      "loss: 0.6511, acc: 0.7316\n",
      "loss: 0.6511, acc: 0.7316\n",
      "> val_acc: 0.6662, val_f1: 0.6314\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.7022, acc: 0.6875\n",
      "loss: 0.6402, acc: 0.7227\n",
      "loss: 0.6302, acc: 0.7380\n",
      "loss: 0.6214, acc: 0.7448\n",
      "loss: 0.6281, acc: 0.7432\n",
      "loss: 0.6232, acc: 0.7444\n",
      "loss: 0.6128, acc: 0.7528\n",
      "loss: 0.6120, acc: 0.7500\n",
      "loss: 0.6201, acc: 0.7435\n",
      "loss: 0.6268, acc: 0.7402\n",
      "loss: 0.6304, acc: 0.7397\n",
      "loss: 0.6288, acc: 0.7411\n",
      "loss: 0.6299, acc: 0.7374\n",
      "loss: 0.6313, acc: 0.7374\n",
      "loss: 0.6335, acc: 0.7363\n",
      "loss: 0.6318, acc: 0.7386\n",
      "loss: 0.6344, acc: 0.7366\n",
      "loss: 0.6361, acc: 0.7349\n",
      "loss: 0.6371, acc: 0.7334\n",
      "loss: 0.6388, acc: 0.7334\n",
      "> val_acc: 0.6763, val_f1: 0.6595\n",
      ">> test_acc: 0.7038, test_f1: 0.6754\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mgan'\n",
    "dataset = 'twitter' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_mgan = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_mgan)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
