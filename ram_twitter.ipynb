{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, ABSADataset\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# adapted from ram.py\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "\n",
    "class RAM(nn.Module):\n",
    "    def locationed_memory(self, memory, memory_len, left_len, aspect_len):\n",
    "        batch_size = memory.shape[0]\n",
    "        seq_len = memory.shape[1]\n",
    "        memory_len = memory_len.cpu().numpy()\n",
    "        left_len = left_len.cpu().numpy()\n",
    "        aspect_len = aspect_len.cpu().numpy()\n",
    "        weight = [[] for i in range(batch_size)]\n",
    "        u = [[] for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            for idx in range(left_len[i]):\n",
    "                weight[i].append(1-(left_len[i]-idx)/memory_len[i])\n",
    "                u[i].append(idx - left_len[i])\n",
    "            for idx in range(left_len[i], left_len[i]+aspect_len[i]):\n",
    "                weight[i].append(1)\n",
    "                u[i].append(0)\n",
    "            for idx in range(left_len[i]+aspect_len[i], memory_len[i]):\n",
    "                weight[i].append(1-(idx-left_len[i]-aspect_len[i]+1)/memory_len[i])\n",
    "                u[i].append(idx-left_len[i]-aspect_len[i]+1)\n",
    "            for idx in range(memory_len[i], seq_len):\n",
    "                weight[i].append(1)\n",
    "                u[i].append(0)\n",
    "\n",
    "        u = torch.tensor(u).float().to(self.opt.device).unsqueeze(2)\n",
    "        weight = torch.tensor(weight).to(self.opt.device).unsqueeze(2)\n",
    "        memory = torch.cat([memory*weight, u], dim=2) \n",
    "   \n",
    "        return memory\n",
    "\n",
    "    def __init__(self, embedding_matrix, opt):\n",
    "        super(RAM, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.bi_lstm_context = DynamicLSTM(opt.embed_dim, opt.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.att_linear = nn.Linear(opt.hidden_dim*2 + 1 + opt.embed_dim*2, 1)\n",
    "        self.gru_cell = nn.GRUCell(opt.hidden_dim*2 + 1, opt.embed_dim)\n",
    "        self.dense = nn.Linear(opt.embed_dim, opt.polarities_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_raw_indices, aspect_indices, text_left_indices = inputs[0], inputs[1], inputs[2]\n",
    "        left_len = torch.sum(text_left_indices != 0, dim=-1)\n",
    "        memory_len = torch.sum(text_raw_indices != 0, dim=-1)\n",
    "        aspect_len = torch.sum(aspect_indices != 0, dim=-1)\n",
    "        nonzeros_aspect = aspect_len.float()\n",
    "\n",
    "        memory = self.embed(text_raw_indices)\n",
    "        memory, (_, _) = self.bi_lstm_context(memory, memory_len)\n",
    "        memory = self.locationed_memory(memory, memory_len, left_len, aspect_len)\n",
    "        \n",
    "        aspect = self.embed(aspect_indices)\n",
    "        aspect = torch.sum(aspect, dim=1)\n",
    "        aspect = torch.div(aspect, nonzeros_aspect.unsqueeze(-1))\n",
    "        et = torch.zeros_like(aspect).to(self.opt.device)\n",
    "\n",
    "        batch_size = memory.size(0)\n",
    "        seq_len = memory.size(1)\n",
    "        for _ in range(self.opt.hops):\n",
    "            g = self.att_linear(torch.cat([memory, \n",
    "                torch.zeros(batch_size, seq_len, self.opt.embed_dim).to(self.opt.device) + et.unsqueeze(1), \n",
    "                torch.zeros(batch_size, seq_len, self.opt.embed_dim).to(self.opt.device) + aspect.unsqueeze(1)], \n",
    "                dim=-1))\n",
    "            alpha = F.softmax(g, dim=1)\n",
    "            i = torch.bmm(alpha.transpose(1, 2), memory).squeeze(1)  \n",
    "            et = self.gru_cell(i, et)\n",
    "        out = self.dense(et)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# adapted from train.py\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "\n",
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "        tokenizer = build_tokenizer(\n",
    "            fnames=[opt.dataset_file['train'], opt.dataset_file['test']],\n",
    "            max_seq_len=opt.max_seq_len,\n",
    "            dat_fname='{0}_tokenizer.dat'.format(opt.dataset))\n",
    "        embedding_matrix = build_embedding_matrix(\n",
    "            word2idx=tokenizer.word2idx,\n",
    "            embed_dim=opt.embed_dim,\n",
    "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))\n",
    "        self.model = opt.model_class(embedding_matrix, opt).to(opt.device)\n",
    "        self.trainset = ABSADataset(opt.dataset_file['train'], tokenizer)\n",
    "        self.testset = ABSADataset(opt.dataset_file['test'], tokenizer)\n",
    "        assert 0 <= opt.valset_ratio < 1\n",
    "        if opt.valset_ratio > 0:\n",
    "            valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
    "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
    "        else:\n",
    "            self.valset = self.testset\n",
    "\n",
    "        if opt.device.type == 'cuda':\n",
    "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
    "        self._print_args()\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        logger.info('> training parameters:')\n",
    "        for arg in vars(self.opt):\n",
    "            logger.info('> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad:\n",
    "                    if len(p.shape) > 1:\n",
    "                        self.opt.initializer(p)\n",
    "                    else:\n",
    "                        stdv = 1. / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
    "        max_val_acc = 0\n",
    "        max_val_f1 = 0\n",
    "        global_step = 0\n",
    "        path = None\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            logger.info(' ' * 100)\n",
    "            logger.info('epoch: {}'.format(epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            # switch model to training mode\n",
    "            self.model.train()\n",
    "            for i_batch, sample_batched in enumerate(train_data_loader):\n",
    "                global_step += 1\n",
    "                # clear gradient accumulators\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = [sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                outputs = self.model(inputs)\n",
    "                targets = sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                loss_total += loss.item() * len(outputs)\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n",
    "            logger.info('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                if not os.path.exists('state_dict'):\n",
    "                    os.mkdir('state_dict')\n",
    "                path = 'state_dict/{0}_{1}_val_acc{2}'.format(self.opt.model_name, self.opt.dataset, round(val_acc, 4))\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "                logger.info('>> saved: {}'.format(path))\n",
    "            if val_f1 > max_val_f1:\n",
    "                max_val_f1 = val_f1\n",
    "\n",
    "        return path\n",
    "\n",
    "    def _evaluate_acc_f1(self, data_loader):\n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "                t_inputs = [t_sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                t_targets = t_sample_batched['polarity'].to(self.opt.device)\n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "        return acc, f1\n",
    "\n",
    "    def run(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = self.opt.optimizer(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "\n",
    "        self._reset_params()\n",
    "        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        self.model.eval()\n",
    "        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n",
    "        logger.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = {\n",
    "    'ram': RAM,\n",
    "}\n",
    "dataset_files = {\n",
    "    'twitter': {\n",
    "        'train': './datasets/acl-14-short-data/train.raw',\n",
    "        'test': './datasets/acl-14-short-data/test.raw'\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'train': './datasets/semeval14/Restaurants_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "    }\n",
    "}\n",
    "input_colses = {\n",
    "    'ram': ['text_raw_indices', 'aspect_indices', 'text_left_indices'],\n",
    "}\n",
    "initializers = {\n",
    "    'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "}\n",
    "optimizers = {\n",
    "    'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "    'adam': torch.optim.Adam,  # default lr=0.001\n",
    "    'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "    'sgd': torch.optim.SGD,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, model_class, dataset_file, inputs_cols, initializer, \n",
    "                 optimizer, model_name, dataset, learning_rate, num_epoch):\n",
    "        self.model_class = model_class\n",
    "        self.dataset_file = dataset_file\n",
    "        self.inputs_cols = inputs_cols\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        \n",
    "        self.dropout = 0.1\n",
    "        self.l2reg = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.log_step = 5\n",
    "        self.embed_dim = 300\n",
    "        self.hidden_dim = 300\n",
    "        self.max_seq_len = 80\n",
    "        self.polarities_dim = 3\n",
    "        self.valset_ratio = 0\n",
    "        self.hops = 3\n",
    "        self.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: twitter_tokenizer.dat\n",
      "loading embedding_matrix: 300_twitter_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.RAM'>\n",
      "> dataset_file: {'train': './datasets/acl-14-short-data/train.raw', 'test': './datasets/acl-14-short-data/test.raw'}\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a14988ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: ram\n",
      "> dataset: twitter\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> hops: 3\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.1028, acc: 0.3937\n",
      "loss: 1.1147, acc: 0.4328\n",
      "loss: 1.0888, acc: 0.4604\n",
      "loss: 1.0790, acc: 0.4570\n",
      "loss: 1.0708, acc: 0.4594\n",
      "loss: 1.0633, acc: 0.4630\n",
      "loss: 1.0584, acc: 0.4692\n",
      "loss: 1.0538, acc: 0.4703\n",
      "loss: 1.0524, acc: 0.4740\n",
      "loss: 1.0453, acc: 0.4806\n",
      "loss: 1.0420, acc: 0.4807\n",
      "loss: 1.0352, acc: 0.4852\n",
      "loss: 1.0296, acc: 0.4906\n",
      "loss: 1.0239, acc: 0.4960\n",
      "loss: 1.0171, acc: 0.5004\n",
      "loss: 1.0125, acc: 0.5027\n",
      "loss: 1.0069, acc: 0.5059\n",
      "loss: 1.0029, acc: 0.5090\n",
      "loss: 0.9997, acc: 0.5115\n",
      "> val_acc: 0.5751, val_f1: 0.4746\n",
      ">> saved: state_dict/ram_twitter_val_acc0.5751\n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "loss: 0.9187, acc: 0.6172\n",
      "loss: 0.8974, acc: 0.6116\n",
      "loss: 0.8790, acc: 0.6146\n",
      "loss: 0.8797, acc: 0.6176\n",
      "loss: 0.8757, acc: 0.6229\n",
      "loss: 0.8707, acc: 0.6233\n",
      "loss: 0.8728, acc: 0.6196\n",
      "loss: 0.8742, acc: 0.6132\n",
      "loss: 0.8713, acc: 0.6112\n",
      "loss: 0.8641, acc: 0.6140\n",
      "loss: 0.8638, acc: 0.6124\n",
      "loss: 0.8638, acc: 0.6121\n",
      "loss: 0.8645, acc: 0.6111\n",
      "loss: 0.8622, acc: 0.6126\n",
      "loss: 0.8598, acc: 0.6141\n",
      "loss: 0.8573, acc: 0.6149\n",
      "loss: 0.8544, acc: 0.6166\n",
      "loss: 0.8471, acc: 0.6200\n",
      "loss: 0.8410, acc: 0.6236\n",
      "loss: 0.8406, acc: 0.6237\n",
      "> val_acc: 0.6474, val_f1: 0.5939\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6474\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.8363, acc: 0.6133\n",
      "loss: 0.8229, acc: 0.6198\n",
      "loss: 0.8301, acc: 0.6150\n",
      "loss: 0.8205, acc: 0.6258\n",
      "loss: 0.8139, acc: 0.6289\n",
      "loss: 0.8000, acc: 0.6433\n",
      "loss: 0.8057, acc: 0.6402\n",
      "loss: 0.8034, acc: 0.6406\n",
      "loss: 0.8015, acc: 0.6449\n",
      "loss: 0.7979, acc: 0.6464\n",
      "loss: 0.7936, acc: 0.6484\n",
      "loss: 0.7955, acc: 0.6480\n",
      "loss: 0.7974, acc: 0.6475\n",
      "loss: 0.7999, acc: 0.6463\n",
      "loss: 0.8022, acc: 0.6451\n",
      "loss: 0.8019, acc: 0.6470\n",
      "loss: 0.7994, acc: 0.6481\n",
      "loss: 0.7995, acc: 0.6487\n",
      "loss: 0.7945, acc: 0.6509\n",
      "> val_acc: 0.6546, val_f1: 0.6290\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6546\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.8091, acc: 0.5625\n",
      "loss: 0.7209, acc: 0.6927\n",
      "loss: 0.7376, acc: 0.6747\n",
      "loss: 0.7492, acc: 0.6758\n",
      "loss: 0.7535, acc: 0.6704\n",
      "loss: 0.7517, acc: 0.6755\n",
      "loss: 0.7506, acc: 0.6734\n",
      "loss: 0.7447, acc: 0.6736\n",
      "loss: 0.7456, acc: 0.6734\n",
      "loss: 0.7496, acc: 0.6688\n",
      "loss: 0.7468, acc: 0.6697\n",
      "loss: 0.7443, acc: 0.6724\n",
      "loss: 0.7457, acc: 0.6726\n",
      "loss: 0.7485, acc: 0.6688\n",
      "loss: 0.7499, acc: 0.6699\n",
      "loss: 0.7537, acc: 0.6696\n",
      "loss: 0.7571, acc: 0.6692\n",
      "loss: 0.7571, acc: 0.6701\n",
      "loss: 0.7537, acc: 0.6731\n",
      "loss: 0.7553, acc: 0.6709\n",
      "> val_acc: 0.6575, val_f1: 0.6370\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6575\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.6722, acc: 0.7344\n",
      "loss: 0.7225, acc: 0.6953\n",
      "loss: 0.7151, acc: 0.6971\n",
      "loss: 0.7202, acc: 0.6970\n",
      "loss: 0.7131, acc: 0.6997\n",
      "loss: 0.7283, acc: 0.6875\n",
      "loss: 0.7359, acc: 0.6813\n",
      "loss: 0.7333, acc: 0.6838\n",
      "loss: 0.7316, acc: 0.6860\n",
      "loss: 0.7284, acc: 0.6904\n",
      "loss: 0.7313, acc: 0.6878\n",
      "loss: 0.7348, acc: 0.6864\n",
      "loss: 0.7365, acc: 0.6848\n",
      "loss: 0.7341, acc: 0.6859\n",
      "loss: 0.7374, acc: 0.6841\n",
      "loss: 0.7395, acc: 0.6831\n",
      "loss: 0.7428, acc: 0.6815\n",
      "loss: 0.7433, acc: 0.6809\n",
      "loss: 0.7449, acc: 0.6804\n",
      "loss: 0.7460, acc: 0.6788\n",
      "> val_acc: 0.6691, val_f1: 0.6308\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6691\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.7215, acc: 0.6687\n",
      "loss: 0.7337, acc: 0.6781\n",
      "loss: 0.7417, acc: 0.6729\n",
      "loss: 0.7386, acc: 0.6766\n",
      "loss: 0.7399, acc: 0.6769\n",
      "loss: 0.7405, acc: 0.6771\n",
      "loss: 0.7418, acc: 0.6763\n",
      "loss: 0.7423, acc: 0.6758\n",
      "loss: 0.7475, acc: 0.6767\n",
      "loss: 0.7422, acc: 0.6816\n",
      "loss: 0.7364, acc: 0.6844\n",
      "loss: 0.7335, acc: 0.6870\n",
      "loss: 0.7275, acc: 0.6885\n",
      "loss: 0.7270, acc: 0.6886\n",
      "loss: 0.7268, acc: 0.6904\n",
      "loss: 0.7246, acc: 0.6918\n",
      "loss: 0.7291, acc: 0.6892\n",
      "loss: 0.7335, acc: 0.6865\n",
      "loss: 0.7311, acc: 0.6885\n",
      "> val_acc: 0.6691, val_f1: 0.6414\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.7088, acc: 0.6641\n",
      "loss: 0.6524, acc: 0.7165\n",
      "loss: 0.6941, acc: 0.6966\n",
      "loss: 0.7084, acc: 0.6857\n",
      "loss: 0.7225, acc: 0.6783\n",
      "loss: 0.7238, acc: 0.6834\n",
      "loss: 0.7193, acc: 0.6865\n",
      "loss: 0.7193, acc: 0.6892\n",
      "loss: 0.7208, acc: 0.6856\n",
      "loss: 0.7178, acc: 0.6868\n",
      "loss: 0.7147, acc: 0.6896\n",
      "loss: 0.7157, acc: 0.6870\n",
      "loss: 0.7219, acc: 0.6840\n",
      "loss: 0.7227, acc: 0.6842\n",
      "loss: 0.7255, acc: 0.6825\n",
      "loss: 0.7255, acc: 0.6828\n",
      "loss: 0.7262, acc: 0.6833\n",
      "loss: 0.7252, acc: 0.6841\n",
      "loss: 0.7247, acc: 0.6868\n",
      "loss: 0.7234, acc: 0.6870\n",
      "> val_acc: 0.6734, val_f1: 0.6390\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6734\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.7259, acc: 0.6836\n",
      "loss: 0.7421, acc: 0.6701\n",
      "loss: 0.7322, acc: 0.6808\n",
      "loss: 0.7237, acc: 0.6834\n",
      "loss: 0.7353, acc: 0.6803\n",
      "loss: 0.7313, acc: 0.6891\n",
      "loss: 0.7292, acc: 0.6903\n",
      "loss: 0.7195, acc: 0.6955\n",
      "loss: 0.7279, acc: 0.6882\n",
      "loss: 0.7219, acc: 0.6913\n",
      "loss: 0.7223, acc: 0.6927\n",
      "loss: 0.7187, acc: 0.6947\n",
      "loss: 0.7213, acc: 0.6946\n",
      "loss: 0.7200, acc: 0.6943\n",
      "loss: 0.7176, acc: 0.6964\n",
      "loss: 0.7136, acc: 0.6986\n",
      "loss: 0.7092, acc: 0.7009\n",
      "loss: 0.7086, acc: 0.7017\n",
      "loss: 0.7078, acc: 0.7015\n",
      "> val_acc: 0.6879, val_f1: 0.6542\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6879\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.6548, acc: 0.7656\n",
      "loss: 0.7192, acc: 0.6979\n",
      "loss: 0.7173, acc: 0.7031\n",
      "loss: 0.7326, acc: 0.6895\n",
      "loss: 0.7308, acc: 0.6927\n",
      "loss: 0.7265, acc: 0.6959\n",
      "loss: 0.7381, acc: 0.6885\n",
      "loss: 0.7372, acc: 0.6888\n",
      "loss: 0.7349, acc: 0.6860\n",
      "loss: 0.7321, acc: 0.6899\n",
      "loss: 0.7232, acc: 0.6939\n",
      "loss: 0.7213, acc: 0.6922\n",
      "loss: 0.7161, acc: 0.6965\n",
      "loss: 0.7152, acc: 0.6967\n",
      "loss: 0.7130, acc: 0.6976\n",
      "loss: 0.7148, acc: 0.6959\n",
      "loss: 0.7087, acc: 0.6998\n",
      "loss: 0.7073, acc: 0.7017\n",
      "loss: 0.7069, acc: 0.7021\n",
      "loss: 0.7074, acc: 0.7021\n",
      "> val_acc: 0.6705, val_f1: 0.6426\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6453, acc: 0.7135\n",
      "loss: 0.6449, acc: 0.7246\n",
      "loss: 0.6957, acc: 0.7031\n",
      "loss: 0.7103, acc: 0.6962\n",
      "loss: 0.7011, acc: 0.6943\n",
      "loss: 0.6991, acc: 0.6975\n",
      "loss: 0.6900, acc: 0.7003\n",
      "loss: 0.6999, acc: 0.6900\n",
      "loss: 0.6964, acc: 0.6948\n",
      "loss: 0.6928, acc: 0.7012\n",
      "loss: 0.6952, acc: 0.7014\n",
      "loss: 0.6925, acc: 0.7045\n",
      "loss: 0.6908, acc: 0.7049\n",
      "loss: 0.6930, acc: 0.7036\n",
      "loss: 0.6967, acc: 0.7021\n",
      "loss: 0.7007, acc: 0.7017\n",
      "loss: 0.7043, acc: 0.6997\n",
      "loss: 0.7104, acc: 0.6967\n",
      "loss: 0.7094, acc: 0.6988\n",
      "loss: 0.7097, acc: 0.6981\n",
      "> val_acc: 0.6792, val_f1: 0.6505\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.6424, acc: 0.7594\n",
      "loss: 0.6514, acc: 0.7531\n",
      "loss: 0.6759, acc: 0.7229\n",
      "loss: 0.6819, acc: 0.7172\n",
      "loss: 0.6870, acc: 0.7156\n",
      "loss: 0.6933, acc: 0.7063\n",
      "loss: 0.6963, acc: 0.7054\n",
      "loss: 0.6963, acc: 0.7063\n",
      "loss: 0.6994, acc: 0.7035\n",
      "loss: 0.6954, acc: 0.7059\n",
      "loss: 0.6970, acc: 0.7048\n",
      "loss: 0.7012, acc: 0.7021\n",
      "loss: 0.6991, acc: 0.7048\n",
      "loss: 0.6954, acc: 0.7063\n",
      "loss: 0.6969, acc: 0.7048\n",
      "loss: 0.6941, acc: 0.7047\n",
      "loss: 0.6956, acc: 0.7051\n",
      "loss: 0.6956, acc: 0.7036\n",
      "loss: 0.6971, acc: 0.7021\n",
      "> val_acc: 0.6691, val_f1: 0.6276\n",
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.6574, acc: 0.7188\n",
      "loss: 0.6763, acc: 0.7321\n",
      "loss: 0.6864, acc: 0.7148\n",
      "loss: 0.6776, acc: 0.7132\n",
      "loss: 0.6749, acc: 0.7095\n",
      "loss: 0.6791, acc: 0.7095\n",
      "loss: 0.6870, acc: 0.7061\n",
      "loss: 0.6948, acc: 0.6976\n",
      "loss: 0.7012, acc: 0.6923\n",
      "loss: 0.7007, acc: 0.6928\n",
      "loss: 0.7003, acc: 0.6941\n",
      "loss: 0.6986, acc: 0.6949\n",
      "loss: 0.7013, acc: 0.6925\n",
      "loss: 0.7046, acc: 0.6910\n",
      "loss: 0.7091, acc: 0.6868\n",
      "loss: 0.7106, acc: 0.6865\n",
      "loss: 0.7109, acc: 0.6867\n",
      "loss: 0.7077, acc: 0.6889\n",
      "loss: 0.7083, acc: 0.6887\n",
      "loss: 0.7076, acc: 0.6904\n",
      "> val_acc: 0.6763, val_f1: 0.6518\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.6875, acc: 0.7344\n",
      "loss: 0.6906, acc: 0.7274\n",
      "loss: 0.7185, acc: 0.7065\n",
      "loss: 0.7251, acc: 0.7007\n",
      "loss: 0.7177, acc: 0.6960\n",
      "loss: 0.7125, acc: 0.6929\n",
      "loss: 0.7175, acc: 0.6912\n",
      "loss: 0.7118, acc: 0.6943\n",
      "loss: 0.7046, acc: 0.7013\n",
      "loss: 0.6996, acc: 0.7044\n",
      "loss: 0.6950, acc: 0.7046\n",
      "loss: 0.6903, acc: 0.7055\n",
      "loss: 0.6906, acc: 0.7048\n",
      "loss: 0.6919, acc: 0.7022\n",
      "loss: 0.6932, acc: 0.7010\n",
      "loss: 0.6930, acc: 0.7025\n",
      "loss: 0.6936, acc: 0.7024\n",
      "loss: 0.6948, acc: 0.7037\n",
      "loss: 0.6919, acc: 0.7056\n",
      "> val_acc: 0.6821, val_f1: 0.6611\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.6918, acc: 0.7344\n",
      "loss: 0.6678, acc: 0.7214\n",
      "loss: 0.6369, acc: 0.7330\n",
      "loss: 0.6404, acc: 0.7285\n",
      "loss: 0.6427, acc: 0.7299\n",
      "loss: 0.6490, acc: 0.7284\n",
      "loss: 0.6544, acc: 0.7263\n",
      "loss: 0.6626, acc: 0.7196\n",
      "loss: 0.6695, acc: 0.7142\n",
      "loss: 0.6687, acc: 0.7157\n",
      "loss: 0.6705, acc: 0.7142\n",
      "loss: 0.6761, acc: 0.7132\n",
      "loss: 0.6748, acc: 0.7134\n",
      "loss: 0.6772, acc: 0.7119\n",
      "loss: 0.6750, acc: 0.7146\n",
      "loss: 0.6744, acc: 0.7155\n",
      "loss: 0.6776, acc: 0.7133\n",
      "loss: 0.6759, acc: 0.7146\n",
      "loss: 0.6734, acc: 0.7167\n",
      "loss: 0.6748, acc: 0.7158\n",
      "> val_acc: 0.6850, val_f1: 0.6596\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.6449, acc: 0.7240\n",
      "loss: 0.6480, acc: 0.7383\n",
      "loss: 0.6398, acc: 0.7416\n",
      "loss: 0.6530, acc: 0.7309\n",
      "loss: 0.6501, acc: 0.7317\n",
      "loss: 0.6515, acc: 0.7321\n",
      "loss: 0.6489, acc: 0.7325\n",
      "loss: 0.6637, acc: 0.7270\n",
      "loss: 0.6776, acc: 0.7166\n",
      "loss: 0.6751, acc: 0.7174\n",
      "loss: 0.6728, acc: 0.7179\n",
      "loss: 0.6704, acc: 0.7182\n",
      "loss: 0.6738, acc: 0.7158\n",
      "loss: 0.6722, acc: 0.7183\n",
      "loss: 0.6726, acc: 0.7177\n",
      "loss: 0.6703, acc: 0.7200\n",
      "loss: 0.6736, acc: 0.7182\n",
      "loss: 0.6759, acc: 0.7170\n",
      "loss: 0.6781, acc: 0.7151\n",
      "loss: 0.6760, acc: 0.7172\n",
      "> val_acc: 0.6922, val_f1: 0.6680\n",
      ">> saved: state_dict/ram_twitter_val_acc0.6922\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.7106, acc: 0.6937\n",
      "loss: 0.7118, acc: 0.6875\n",
      "loss: 0.7083, acc: 0.6948\n",
      "loss: 0.6978, acc: 0.7008\n",
      "loss: 0.6978, acc: 0.7037\n",
      "loss: 0.6924, acc: 0.7057\n",
      "loss: 0.6939, acc: 0.7049\n",
      "loss: 0.6906, acc: 0.7063\n",
      "loss: 0.6887, acc: 0.7063\n",
      "loss: 0.6829, acc: 0.7087\n",
      "loss: 0.6751, acc: 0.7131\n",
      "loss: 0.6741, acc: 0.7128\n",
      "loss: 0.6777, acc: 0.7087\n",
      "loss: 0.6783, acc: 0.7080\n",
      "loss: 0.6800, acc: 0.7075\n",
      "loss: 0.6806, acc: 0.7076\n",
      "loss: 0.6823, acc: 0.7074\n",
      "loss: 0.6803, acc: 0.7071\n",
      "loss: 0.6845, acc: 0.7058\n",
      "> val_acc: 0.6893, val_f1: 0.6803\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.7630, acc: 0.7031\n",
      "loss: 0.6903, acc: 0.7277\n",
      "loss: 0.7032, acc: 0.7083\n",
      "loss: 0.6632, acc: 0.7243\n",
      "loss: 0.6724, acc: 0.7202\n",
      "loss: 0.6673, acc: 0.7269\n",
      "loss: 0.6707, acc: 0.7217\n",
      "loss: 0.6666, acc: 0.7230\n",
      "loss: 0.6672, acc: 0.7240\n",
      "loss: 0.6661, acc: 0.7251\n",
      "loss: 0.6621, acc: 0.7260\n",
      "loss: 0.6604, acc: 0.7267\n",
      "loss: 0.6596, acc: 0.7240\n",
      "loss: 0.6597, acc: 0.7250\n",
      "loss: 0.6630, acc: 0.7224\n",
      "loss: 0.6638, acc: 0.7218\n",
      "loss: 0.6615, acc: 0.7233\n",
      "loss: 0.6633, acc: 0.7222\n",
      "loss: 0.6632, acc: 0.7218\n",
      "loss: 0.6641, acc: 0.7207\n",
      "> val_acc: 0.6734, val_f1: 0.6507\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.6476, acc: 0.7148\n",
      "loss: 0.6476, acc: 0.7274\n",
      "loss: 0.6610, acc: 0.7277\n",
      "loss: 0.6479, acc: 0.7368\n",
      "loss: 0.6395, acc: 0.7435\n",
      "loss: 0.6344, acc: 0.7435\n",
      "loss: 0.6348, acc: 0.7426\n",
      "loss: 0.6408, acc: 0.7384\n",
      "loss: 0.6402, acc: 0.7362\n",
      "loss: 0.6521, acc: 0.7312\n",
      "loss: 0.6547, acc: 0.7318\n",
      "loss: 0.6586, acc: 0.7288\n",
      "loss: 0.6634, acc: 0.7271\n",
      "loss: 0.6637, acc: 0.7251\n",
      "loss: 0.6618, acc: 0.7264\n",
      "loss: 0.6641, acc: 0.7233\n",
      "loss: 0.6658, acc: 0.7223\n",
      "loss: 0.6668, acc: 0.7219\n",
      "loss: 0.6674, acc: 0.7212\n",
      "> val_acc: 0.6777, val_f1: 0.6546\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.5678, acc: 0.8125\n",
      "loss: 0.6375, acc: 0.7396\n",
      "loss: 0.6348, acc: 0.7443\n",
      "loss: 0.6318, acc: 0.7363\n",
      "loss: 0.6351, acc: 0.7307\n",
      "loss: 0.6315, acc: 0.7326\n",
      "loss: 0.6453, acc: 0.7278\n",
      "loss: 0.6557, acc: 0.7218\n",
      "loss: 0.6589, acc: 0.7195\n",
      "loss: 0.6575, acc: 0.7225\n",
      "loss: 0.6615, acc: 0.7184\n",
      "loss: 0.6596, acc: 0.7215\n",
      "loss: 0.6624, acc: 0.7182\n",
      "loss: 0.6570, acc: 0.7228\n",
      "loss: 0.6574, acc: 0.7223\n",
      "loss: 0.6568, acc: 0.7231\n",
      "loss: 0.6594, acc: 0.7216\n",
      "loss: 0.6586, acc: 0.7224\n",
      "loss: 0.6624, acc: 0.7206\n",
      "loss: 0.6633, acc: 0.7205\n",
      "> val_acc: 0.6835, val_f1: 0.6632\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.6015, acc: 0.7188\n",
      "loss: 0.6437, acc: 0.7168\n",
      "loss: 0.6387, acc: 0.7260\n",
      "loss: 0.6556, acc: 0.7214\n",
      "loss: 0.6508, acc: 0.7221\n",
      "loss: 0.6512, acc: 0.7193\n",
      "loss: 0.6486, acc: 0.7211\n",
      "loss: 0.6534, acc: 0.7188\n",
      "loss: 0.6563, acc: 0.7169\n",
      "loss: 0.6567, acc: 0.7171\n",
      "loss: 0.6580, acc: 0.7152\n",
      "loss: 0.6608, acc: 0.7128\n",
      "loss: 0.6621, acc: 0.7108\n",
      "loss: 0.6582, acc: 0.7132\n",
      "loss: 0.6564, acc: 0.7147\n",
      "loss: 0.6588, acc: 0.7123\n",
      "loss: 0.6579, acc: 0.7133\n",
      "loss: 0.6609, acc: 0.7129\n",
      "loss: 0.6566, acc: 0.7154\n",
      "loss: 0.6566, acc: 0.7154\n",
      "> val_acc: 0.6908, val_f1: 0.6638\n",
      ">> test_acc: 0.6922, test_f1: 0.6680\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ram'\n",
    "dataset = 'twitter' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_lstm = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_lstm)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
