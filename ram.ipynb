{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, ABSADataset\n",
    "from layers.dynamic_rnn import DynamicLSTM\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# file: ram.py\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "\n",
    "class RAM(nn.Module):\n",
    "    def locationed_memory(self, memory, memory_len, left_len, aspect_len):\n",
    "        batch_size = memory.shape[0]\n",
    "        seq_len = memory.shape[1]\n",
    "        memory_len = memory_len.cpu().numpy()\n",
    "        left_len = left_len.cpu().numpy()\n",
    "        aspect_len = aspect_len.cpu().numpy()\n",
    "        weight = [[] for i in range(batch_size)]\n",
    "        u = [[] for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            for idx in range(left_len[i]):\n",
    "                weight[i].append(1-(left_len[i]-idx)/memory_len[i])\n",
    "                u[i].append(idx - left_len[i])\n",
    "            for idx in range(left_len[i], left_len[i]+aspect_len[i]):\n",
    "                weight[i].append(1)\n",
    "                u[i].append(0)\n",
    "            for idx in range(left_len[i]+aspect_len[i], memory_len[i]):\n",
    "                weight[i].append(1-(idx-left_len[i]-aspect_len[i]+1)/memory_len[i])\n",
    "                u[i].append(idx-left_len[i]-aspect_len[i]+1)\n",
    "            for idx in range(memory_len[i], seq_len):\n",
    "                weight[i].append(1)\n",
    "                u[i].append(0)\n",
    "\n",
    "        u = torch.tensor(u).float().to(self.opt.device).unsqueeze(2)\n",
    "        weight = torch.tensor(weight).to(self.opt.device).unsqueeze(2)\n",
    "        memory = torch.cat([memory*weight, u], dim=2) \n",
    "   \n",
    "        return memory\n",
    "\n",
    "    def __init__(self, embedding_matrix, opt):\n",
    "        super(RAM, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.bi_lstm_context = DynamicLSTM(opt.embed_dim, opt.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.att_linear = nn.Linear(opt.hidden_dim*2 + 1 + opt.embed_dim*2, 1)\n",
    "        self.gru_cell = nn.GRUCell(opt.hidden_dim*2 + 1, opt.embed_dim)\n",
    "        self.dense = nn.Linear(opt.embed_dim, opt.polarities_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_raw_indices, aspect_indices, text_left_indices = inputs[0], inputs[1], inputs[2]\n",
    "        left_len = torch.sum(text_left_indices != 0, dim=-1)\n",
    "        memory_len = torch.sum(text_raw_indices != 0, dim=-1)\n",
    "        aspect_len = torch.sum(aspect_indices != 0, dim=-1)\n",
    "        nonzeros_aspect = aspect_len.float()\n",
    "\n",
    "        memory = self.embed(text_raw_indices)\n",
    "        memory, (_, _) = self.bi_lstm_context(memory, memory_len)\n",
    "        memory = self.locationed_memory(memory, memory_len, left_len, aspect_len)\n",
    "        \n",
    "        aspect = self.embed(aspect_indices)\n",
    "        aspect = torch.sum(aspect, dim=1)\n",
    "        aspect = torch.div(aspect, nonzeros_aspect.unsqueeze(-1))\n",
    "        et = torch.zeros_like(aspect).to(self.opt.device)\n",
    "\n",
    "        batch_size = memory.size(0)\n",
    "        seq_len = memory.size(1)\n",
    "        for _ in range(self.opt.hops):\n",
    "            g = self.att_linear(torch.cat([memory, \n",
    "                torch.zeros(batch_size, seq_len, self.opt.embed_dim).to(self.opt.device) + et.unsqueeze(1), \n",
    "                torch.zeros(batch_size, seq_len, self.opt.embed_dim).to(self.opt.device) + aspect.unsqueeze(1)], \n",
    "                dim=-1))\n",
    "            alpha = F.softmax(g, dim=1)\n",
    "            i = torch.bmm(alpha.transpose(1, 2), memory).squeeze(1)  \n",
    "            et = self.gru_cell(i, et)\n",
    "        out = self.dense(et)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# file: train.py\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "\n",
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "        tokenizer = build_tokenizer(\n",
    "            fnames=[opt.dataset_file['train'], opt.dataset_file['test']],\n",
    "            max_seq_len=opt.max_seq_len,\n",
    "            dat_fname='{0}_tokenizer.dat'.format(opt.dataset))\n",
    "        embedding_matrix = build_embedding_matrix(\n",
    "            word2idx=tokenizer.word2idx,\n",
    "            embed_dim=opt.embed_dim,\n",
    "            dat_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset))\n",
    "        self.model = opt.model_class(embedding_matrix, opt).to(opt.device)\n",
    "        self.trainset = ABSADataset(opt.dataset_file['train'], tokenizer)\n",
    "        self.testset = ABSADataset(opt.dataset_file['test'], tokenizer)\n",
    "        assert 0 <= opt.valset_ratio < 1\n",
    "        if opt.valset_ratio > 0:\n",
    "            valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
    "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
    "        else:\n",
    "            self.valset = self.testset\n",
    "\n",
    "        if opt.device.type == 'cuda':\n",
    "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
    "        self._print_args()\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        logger.info('> training parameters:')\n",
    "        for arg in vars(self.opt):\n",
    "            logger.info('> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad:\n",
    "                    if len(p.shape) > 1:\n",
    "                        self.opt.initializer(p)\n",
    "                    else:\n",
    "                        stdv = 1. / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
    "        max_val_acc = 0\n",
    "        max_val_f1 = 0\n",
    "        global_step = 0\n",
    "        path = None\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            logger.info(' ' * 100)\n",
    "            logger.info('epoch: {}'.format(epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            # switch model to training mode\n",
    "            self.model.train()\n",
    "            for i_batch, sample_batched in enumerate(train_data_loader):\n",
    "                global_step += 1\n",
    "                # clear gradient accumulators\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = [sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                outputs = self.model(inputs)\n",
    "                targets = sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                loss_total += loss.item() * len(outputs)\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "            val_acc, val_f1 = self._evaluate_acc_f1(val_data_loader)\n",
    "            logger.info('> val_acc: {:.4f}, val_f1: {:.4f}'.format(val_acc, val_f1))\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                if not os.path.exists('state_dict'):\n",
    "                    os.mkdir('state_dict')\n",
    "                path = 'state_dict/{0}_{1}_val_acc{2}'.format(self.opt.model_name, self.opt.dataset, round(val_acc, 4))\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "                logger.info('>> saved: {}'.format(path))\n",
    "            if val_f1 > max_val_f1:\n",
    "                max_val_f1 = val_f1\n",
    "\n",
    "        return path\n",
    "\n",
    "    def _evaluate_acc_f1(self, data_loader):\n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "                t_inputs = [t_sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                t_targets = t_sample_batched['polarity'].to(self.opt.device)\n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "        return acc, f1\n",
    "\n",
    "    def run(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = self.opt.optimizer(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=False)\n",
    "\n",
    "        self._reset_params()\n",
    "        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader)\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        self.model.eval()\n",
    "        test_acc, test_f1 = self._evaluate_acc_f1(test_data_loader)\n",
    "        logger.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = {\n",
    "    'ram': RAM,\n",
    "}\n",
    "dataset_files = {\n",
    "    'twitter': {\n",
    "        'train': './datasets/acl-14-short-data/train.raw',\n",
    "        'test': './datasets/acl-14-short-data/test.raw'\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'train': './datasets/semeval14/Restaurants_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "    }\n",
    "}\n",
    "input_colses = {\n",
    "    'ram': ['text_raw_indices', 'aspect_indices', 'text_left_indices'],\n",
    "}\n",
    "initializers = {\n",
    "    'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "}\n",
    "optimizers = {\n",
    "    'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "    'adam': torch.optim.Adam,  # default lr=0.001\n",
    "    'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "    'sgd': torch.optim.SGD,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, model_class, dataset_file, inputs_cols, initializer, \n",
    "                 optimizer, model_name, dataset, learning_rate, num_epoch):\n",
    "        self.model_class = model_class\n",
    "        self.dataset_file = dataset_file\n",
    "        self.inputs_cols = inputs_cols\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epoch = num_epoch\n",
    "        \n",
    "        self.dropout = 0.1\n",
    "        self.l2reg = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.log_step = 5\n",
    "        self.embed_dim = 300\n",
    "        self.hidden_dim = 300\n",
    "        self.max_seq_len = 80\n",
    "        self.polarities_dim = 3\n",
    "        self.valset_ratio = 0\n",
    "        self.hops = 3\n",
    "        self.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: laptop_tokenizer.dat\n",
      "loading embedding_matrix: 300_laptop_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> training parameters:\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.RAM'>\n",
      "> model_class: <class '__main__.RAM'>\n",
      "> model_class: <class '__main__.RAM'>\n",
      "> dataset_file: {'train': './datasets/semeval14/Laptops_Train.xml.seg', 'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'}\n",
      "> dataset_file: {'train': './datasets/semeval14/Laptops_Train.xml.seg', 'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'}\n",
      "> dataset_file: {'train': './datasets/semeval14/Laptops_Train.xml.seg', 'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'}\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> inputs_cols: ['text_raw_indices', 'aspect_indices', 'text_left_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a1cd05b70>\n",
      "> initializer: <function xavier_uniform_ at 0x1a1cd05b70>\n",
      "> initializer: <function xavier_uniform_ at 0x1a1cd05b70>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: ram\n",
      "> model_name: ram\n",
      "> model_name: ram\n",
      "> dataset: laptop\n",
      "> dataset: laptop\n",
      "> dataset: laptop\n",
      "> learning_rate: 0.001\n",
      "> learning_rate: 0.001\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> num_epoch: 20\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> dropout: 0.1\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> l2reg: 0.01\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> batch_size: 64\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> log_step: 5\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> embed_dim: 300\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> hidden_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> max_seq_len: 80\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> polarities_dim: 3\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> valset_ratio: 0\n",
      "> valset_ratio: 0\n",
      "> hops: 3\n",
      "> hops: 3\n",
      "> hops: 3\n",
      "> device: cpu\n",
      "> device: cpu\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "                                                                                                    \n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "epoch: 0\n",
      "epoch: 0\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0731, acc: 0.4500\n",
      "loss: 1.0731, acc: 0.4500\n",
      "loss: 1.0731, acc: 0.4500\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0683, acc: 0.4500\n",
      "loss: 1.0683, acc: 0.4500\n",
      "loss: 1.0683, acc: 0.4500\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0597, acc: 0.4531\n",
      "loss: 1.0597, acc: 0.4531\n",
      "loss: 1.0597, acc: 0.4531\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0576, acc: 0.4492\n",
      "loss: 1.0576, acc: 0.4492\n",
      "loss: 1.0576, acc: 0.4492\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0554, acc: 0.4487\n",
      "loss: 1.0554, acc: 0.4487\n",
      "loss: 1.0554, acc: 0.4487\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0529, acc: 0.4536\n",
      "loss: 1.0529, acc: 0.4536\n",
      "loss: 1.0529, acc: 0.4536\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0487, acc: 0.4567\n",
      "loss: 1.0487, acc: 0.4567\n",
      "loss: 1.0487, acc: 0.4567\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "> val_acc: 0.4342, val_f1: 0.3161\n",
      "> val_acc: 0.4342, val_f1: 0.3161\n",
      "> val_acc: 0.4342, val_f1: 0.3161\n",
      ">> saved: state_dict/ram_laptop_val_acc0.4342\n",
      ">> saved: state_dict/ram_laptop_val_acc0.4342\n",
      ">> saved: state_dict/ram_laptop_val_acc0.4342\n",
      "                                                                                                    \n",
      "                                                                                                    \n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "epoch: 1\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0395, acc: 0.4896\n",
      "loss: 1.0395, acc: 0.4896\n",
      "loss: 1.0395, acc: 0.4896\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 0.9969, acc: 0.5469\n",
      "loss: 0.9969, acc: 0.5469\n",
      "loss: 0.9969, acc: 0.5469\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0110, acc: 0.5276\n",
      "loss: 1.0110, acc: 0.5276\n",
      "loss: 1.0110, acc: 0.5276\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0065, acc: 0.5304\n",
      "loss: 1.0065, acc: 0.5304\n",
      "loss: 1.0065, acc: 0.5304\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 1.0033, acc: 0.5340\n",
      "loss: 1.0033, acc: 0.5340\n",
      "loss: 1.0033, acc: 0.5340\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 0.9905, acc: 0.5374\n",
      "loss: 0.9905, acc: 0.5374\n",
      "loss: 0.9905, acc: 0.5374\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "loss: 0.9897, acc: 0.5374\n",
      "loss: 0.9897, acc: 0.5374\n",
      "loss: 0.9897, acc: 0.5374\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-c68cb877fac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_ram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-e979419c5ff1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-e979419c5ff1>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, criterion, optimizer, train_data_loader, val_data_loader)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = 'ram'\n",
    "dataset = 'laptop' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_ram = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_ram)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ram'\n",
    "dataset = 'restaurant' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_lstm = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_lstm)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: twitter_tokenizer.dat\n",
      "loading embedding_matrix: 300_twitter_embedding_matrix.dat\n",
      "> training parameters:\n",
      "> model_class: <class '__main__.LSTM'>\n",
      "> dataset_file: {'train': './datasets/acl-14-short-data/train.raw', 'test': './datasets/acl-14-short-data/test.raw'}\n",
      "> inputs_cols: ['text_raw_indices']\n",
      "> initializer: <function xavier_uniform_ at 0x1a18039ae8>\n",
      "> optimizer: <class 'torch.optim.adam.Adam'>\n",
      "> model_name: lstm\n",
      "> dataset: twitter\n",
      "> learning_rate: 0.001\n",
      "> num_epoch: 20\n",
      "> dropout: 0.1\n",
      "> l2reg: 0.01\n",
      "> batch_size: 64\n",
      "> log_step: 5\n",
      "> embed_dim: 300\n",
      "> hidden_dim: 300\n",
      "> max_seq_len: 80\n",
      "> polarities_dim: 3\n",
      "> valset_ratio: 0\n",
      "> device: cpu\n",
      "                                                                                                    \n",
      "epoch: 0\n",
      "loss: 1.0440, acc: 0.4719\n",
      "loss: 1.0530, acc: 0.4719\n",
      "loss: 1.0364, acc: 0.4948\n",
      "loss: 1.0413, acc: 0.4914\n",
      "loss: 1.0330, acc: 0.4950\n",
      "loss: 1.0308, acc: 0.4927\n",
      "loss: 1.0265, acc: 0.4942\n",
      "loss: 1.0173, acc: 0.5012\n",
      "loss: 1.0141, acc: 0.5017\n",
      "loss: 1.0062, acc: 0.5075\n",
      "loss: 1.0026, acc: 0.5071\n",
      "loss: 0.9961, acc: 0.5091\n",
      "loss: 0.9923, acc: 0.5125\n",
      "loss: 0.9884, acc: 0.5156\n",
      "loss: 0.9849, acc: 0.5179\n",
      "loss: 0.9826, acc: 0.5211\n",
      "loss: 0.9777, acc: 0.5270\n",
      "loss: 0.9727, acc: 0.5267\n",
      "loss: 0.9683, acc: 0.5317\n",
      "> val_acc: 0.6055, val_f1: 0.5571\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6055\n",
      "                                                                                                    \n",
      "epoch: 1\n",
      "loss: 0.8126, acc: 0.5703\n",
      "loss: 0.8050, acc: 0.6049\n",
      "loss: 0.8262, acc: 0.6016\n",
      "loss: 0.8558, acc: 0.5836\n",
      "loss: 0.8683, acc: 0.5824\n",
      "loss: 0.8703, acc: 0.5862\n",
      "loss: 0.8768, acc: 0.5845\n",
      "loss: 0.8752, acc: 0.5849\n",
      "loss: 0.8726, acc: 0.5878\n",
      "loss: 0.8734, acc: 0.5878\n",
      "loss: 0.8724, acc: 0.5916\n",
      "loss: 0.8759, acc: 0.5858\n",
      "loss: 0.8760, acc: 0.5862\n",
      "loss: 0.8778, acc: 0.5842\n",
      "loss: 0.8769, acc: 0.5862\n",
      "loss: 0.8746, acc: 0.5883\n",
      "loss: 0.8708, acc: 0.5897\n",
      "loss: 0.8679, acc: 0.5911\n",
      "loss: 0.8667, acc: 0.5932\n",
      "loss: 0.8668, acc: 0.5931\n",
      "> val_acc: 0.6113, val_f1: 0.5598\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6113\n",
      "                                                                                                    \n",
      "epoch: 2\n",
      "loss: 0.8748, acc: 0.5703\n",
      "loss: 0.8599, acc: 0.5885\n",
      "loss: 0.8485, acc: 0.6004\n",
      "loss: 0.8500, acc: 0.6044\n",
      "loss: 0.8368, acc: 0.6159\n",
      "loss: 0.8336, acc: 0.6185\n",
      "loss: 0.8310, acc: 0.6241\n",
      "loss: 0.8264, acc: 0.6254\n",
      "loss: 0.8281, acc: 0.6246\n",
      "loss: 0.8314, acc: 0.6173\n",
      "loss: 0.8317, acc: 0.6169\n",
      "loss: 0.8311, acc: 0.6178\n",
      "loss: 0.8301, acc: 0.6172\n",
      "loss: 0.8294, acc: 0.6184\n",
      "loss: 0.8259, acc: 0.6227\n",
      "loss: 0.8235, acc: 0.6256\n",
      "loss: 0.8218, acc: 0.6267\n",
      "loss: 0.8254, acc: 0.6243\n",
      "loss: 0.8265, acc: 0.6222\n",
      "> val_acc: 0.6214, val_f1: 0.5613\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6214\n",
      "                                                                                                    \n",
      "epoch: 3\n",
      "loss: 0.7961, acc: 0.6094\n",
      "loss: 0.8098, acc: 0.6328\n",
      "loss: 0.7988, acc: 0.6449\n",
      "loss: 0.8131, acc: 0.6299\n",
      "loss: 0.8157, acc: 0.6176\n",
      "loss: 0.8052, acc: 0.6256\n",
      "loss: 0.7972, acc: 0.6346\n",
      "loss: 0.8023, acc: 0.6363\n",
      "loss: 0.7982, acc: 0.6361\n",
      "loss: 0.8098, acc: 0.6270\n",
      "loss: 0.8145, acc: 0.6244\n",
      "loss: 0.8195, acc: 0.6214\n",
      "loss: 0.8188, acc: 0.6204\n",
      "loss: 0.8145, acc: 0.6219\n",
      "loss: 0.8124, acc: 0.6221\n",
      "loss: 0.8122, acc: 0.6236\n",
      "loss: 0.8162, acc: 0.6240\n",
      "loss: 0.8204, acc: 0.6225\n",
      "loss: 0.8172, acc: 0.6252\n",
      "loss: 0.8150, acc: 0.6260\n",
      "> val_acc: 0.6402, val_f1: 0.6197\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6402\n",
      "                                                                                                    \n",
      "epoch: 4\n",
      "loss: 0.8395, acc: 0.5938\n",
      "loss: 0.8032, acc: 0.6289\n",
      "loss: 0.7663, acc: 0.6550\n",
      "loss: 0.8060, acc: 0.6354\n",
      "loss: 0.8073, acc: 0.6318\n",
      "loss: 0.7911, acc: 0.6473\n",
      "loss: 0.7946, acc: 0.6491\n",
      "loss: 0.7970, acc: 0.6456\n",
      "loss: 0.7914, acc: 0.6504\n",
      "loss: 0.7869, acc: 0.6533\n",
      "loss: 0.7886, acc: 0.6515\n",
      "loss: 0.7850, acc: 0.6546\n",
      "loss: 0.7858, acc: 0.6543\n",
      "loss: 0.7888, acc: 0.6521\n",
      "loss: 0.7914, acc: 0.6505\n",
      "loss: 0.7948, acc: 0.6486\n",
      "loss: 0.7948, acc: 0.6482\n",
      "loss: 0.7934, acc: 0.6490\n",
      "loss: 0.7923, acc: 0.6507\n",
      "loss: 0.7953, acc: 0.6492\n",
      "> val_acc: 0.6445, val_f1: 0.6078\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6445\n",
      "                                                                                                    \n",
      "epoch: 5\n",
      "loss: 0.7561, acc: 0.6750\n",
      "loss: 0.7645, acc: 0.6672\n",
      "loss: 0.7690, acc: 0.6667\n",
      "loss: 0.7687, acc: 0.6687\n",
      "loss: 0.7732, acc: 0.6613\n",
      "loss: 0.7734, acc: 0.6526\n",
      "loss: 0.7771, acc: 0.6442\n",
      "loss: 0.7792, acc: 0.6449\n",
      "loss: 0.7798, acc: 0.6444\n",
      "loss: 0.7795, acc: 0.6450\n",
      "loss: 0.7832, acc: 0.6438\n",
      "loss: 0.7789, acc: 0.6492\n",
      "loss: 0.7762, acc: 0.6517\n",
      "loss: 0.7756, acc: 0.6516\n",
      "loss: 0.7736, acc: 0.6544\n",
      "loss: 0.7756, acc: 0.6545\n",
      "loss: 0.7777, acc: 0.6535\n",
      "loss: 0.7780, acc: 0.6540\n",
      "loss: 0.7779, acc: 0.6541\n",
      "> val_acc: 0.6532, val_f1: 0.6201\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6532\n",
      "                                                                                                    \n",
      "epoch: 6\n",
      "loss: 0.6990, acc: 0.7344\n",
      "loss: 0.7421, acc: 0.6853\n",
      "loss: 0.7585, acc: 0.6732\n",
      "loss: 0.7582, acc: 0.6719\n",
      "loss: 0.7667, acc: 0.6641\n",
      "loss: 0.7603, acc: 0.6678\n",
      "loss: 0.7563, acc: 0.6689\n",
      "loss: 0.7559, acc: 0.6689\n",
      "loss: 0.7657, acc: 0.6670\n",
      "loss: 0.7621, acc: 0.6705\n",
      "loss: 0.7592, acc: 0.6740\n",
      "loss: 0.7565, acc: 0.6743\n",
      "loss: 0.7587, acc: 0.6726\n",
      "loss: 0.7602, acc: 0.6700\n",
      "loss: 0.7638, acc: 0.6680\n",
      "loss: 0.7646, acc: 0.6668\n",
      "loss: 0.7666, acc: 0.6662\n",
      "loss: 0.7678, acc: 0.6649\n",
      "loss: 0.7676, acc: 0.6639\n",
      "loss: 0.7667, acc: 0.6649\n",
      "> val_acc: 0.6618, val_f1: 0.6144\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6618\n",
      "                                                                                                    \n",
      "epoch: 7\n",
      "loss: 0.7181, acc: 0.6992\n",
      "loss: 0.7292, acc: 0.6806\n",
      "loss: 0.7399, acc: 0.6696\n",
      "loss: 0.7533, acc: 0.6604\n",
      "loss: 0.7503, acc: 0.6654\n",
      "loss: 0.7511, acc: 0.6708\n",
      "loss: 0.7472, acc: 0.6737\n",
      "loss: 0.7509, acc: 0.6719\n",
      "loss: 0.7543, acc: 0.6722\n",
      "loss: 0.7606, acc: 0.6680\n",
      "loss: 0.7599, acc: 0.6681\n",
      "loss: 0.7587, acc: 0.6682\n",
      "loss: 0.7569, acc: 0.6689\n",
      "loss: 0.7572, acc: 0.6707\n",
      "loss: 0.7540, acc: 0.6744\n",
      "loss: 0.7517, acc: 0.6744\n",
      "loss: 0.7576, acc: 0.6709\n",
      "loss: 0.7590, acc: 0.6698\n",
      "loss: 0.7584, acc: 0.6697\n",
      "> val_acc: 0.6315, val_f1: 0.5695\n",
      "                                                                                                    \n",
      "epoch: 8\n",
      "loss: 0.6937, acc: 0.6719\n",
      "loss: 0.7588, acc: 0.6536\n",
      "loss: 0.7627, acc: 0.6577\n",
      "loss: 0.7509, acc: 0.6602\n",
      "loss: 0.7463, acc: 0.6659\n",
      "loss: 0.7389, acc: 0.6707\n",
      "loss: 0.7356, acc: 0.6794\n",
      "loss: 0.7358, acc: 0.6771\n",
      "loss: 0.7388, acc: 0.6768\n",
      "loss: 0.7396, acc: 0.6760\n",
      "loss: 0.7369, acc: 0.6783\n",
      "loss: 0.7353, acc: 0.6791\n",
      "loss: 0.7414, acc: 0.6742\n",
      "loss: 0.7438, acc: 0.6738\n",
      "loss: 0.7466, acc: 0.6710\n",
      "loss: 0.7485, acc: 0.6702\n",
      "loss: 0.7489, acc: 0.6696\n",
      "loss: 0.7492, acc: 0.6697\n",
      "loss: 0.7470, acc: 0.6703\n",
      "loss: 0.7462, acc: 0.6711\n",
      "> val_acc: 0.6662, val_f1: 0.6374\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6662\n",
      "                                                                                                    \n",
      "epoch: 9\n",
      "loss: 0.6880, acc: 0.7031\n",
      "loss: 0.7129, acc: 0.6855\n",
      "loss: 0.7220, acc: 0.6911\n",
      "loss: 0.7147, acc: 0.7014\n",
      "loss: 0.7174, acc: 0.7024\n",
      "loss: 0.7244, acc: 0.6931\n",
      "loss: 0.7181, acc: 0.6979\n",
      "loss: 0.7172, acc: 0.6994\n",
      "loss: 0.7160, acc: 0.7002\n",
      "loss: 0.7168, acc: 0.7002\n",
      "loss: 0.7146, acc: 0.6990\n",
      "loss: 0.7216, acc: 0.6953\n",
      "loss: 0.7263, acc: 0.6902\n",
      "loss: 0.7268, acc: 0.6907\n",
      "loss: 0.7300, acc: 0.6890\n",
      "loss: 0.7359, acc: 0.6827\n",
      "loss: 0.7356, acc: 0.6830\n",
      "loss: 0.7348, acc: 0.6832\n",
      "loss: 0.7401, acc: 0.6794\n",
      "loss: 0.7441, acc: 0.6761\n",
      "> val_acc: 0.6633, val_f1: 0.6314\n",
      "                                                                                                    \n",
      "epoch: 10\n",
      "loss: 0.7913, acc: 0.6781\n",
      "loss: 0.7623, acc: 0.6937\n",
      "loss: 0.7573, acc: 0.6937\n",
      "loss: 0.7578, acc: 0.6953\n",
      "loss: 0.7466, acc: 0.6994\n",
      "loss: 0.7450, acc: 0.6948\n",
      "loss: 0.7420, acc: 0.6915\n",
      "loss: 0.7374, acc: 0.6937\n",
      "loss: 0.7321, acc: 0.6931\n",
      "loss: 0.7290, acc: 0.6931\n",
      "loss: 0.7307, acc: 0.6926\n",
      "loss: 0.7281, acc: 0.6937\n",
      "loss: 0.7284, acc: 0.6928\n",
      "loss: 0.7252, acc: 0.6924\n",
      "loss: 0.7293, acc: 0.6915\n",
      "loss: 0.7317, acc: 0.6906\n",
      "loss: 0.7312, acc: 0.6914\n",
      "loss: 0.7292, acc: 0.6920\n",
      "loss: 0.7303, acc: 0.6901\n",
      "> val_acc: 0.6358, val_f1: 0.6173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \n",
      "epoch: 11\n",
      "loss: 0.6216, acc: 0.7578\n",
      "loss: 0.6860, acc: 0.7366\n",
      "loss: 0.7144, acc: 0.7083\n",
      "loss: 0.7118, acc: 0.7050\n",
      "loss: 0.7247, acc: 0.7010\n",
      "loss: 0.7217, acc: 0.7008\n",
      "loss: 0.7179, acc: 0.7017\n",
      "loss: 0.7163, acc: 0.7023\n",
      "loss: 0.7145, acc: 0.7024\n",
      "loss: 0.7075, acc: 0.7058\n",
      "loss: 0.7075, acc: 0.7043\n",
      "loss: 0.7052, acc: 0.7053\n",
      "loss: 0.7091, acc: 0.7021\n",
      "loss: 0.7120, acc: 0.7001\n",
      "loss: 0.7162, acc: 0.6986\n",
      "loss: 0.7163, acc: 0.6991\n",
      "loss: 0.7152, acc: 0.6997\n",
      "loss: 0.7209, acc: 0.6952\n",
      "loss: 0.7214, acc: 0.6948\n",
      "loss: 0.7228, acc: 0.6933\n",
      "> val_acc: 0.6575, val_f1: 0.6013\n",
      "                                                                                                    \n",
      "epoch: 12\n",
      "loss: 0.7464, acc: 0.6836\n",
      "loss: 0.7227, acc: 0.6823\n",
      "loss: 0.7247, acc: 0.6830\n",
      "loss: 0.7269, acc: 0.6891\n",
      "loss: 0.7140, acc: 0.6960\n",
      "loss: 0.7127, acc: 0.6977\n",
      "loss: 0.7100, acc: 0.6958\n",
      "loss: 0.7079, acc: 0.7003\n",
      "loss: 0.7088, acc: 0.6996\n",
      "loss: 0.7164, acc: 0.6939\n",
      "loss: 0.7131, acc: 0.6976\n",
      "loss: 0.7130, acc: 0.6970\n",
      "loss: 0.7118, acc: 0.6982\n",
      "loss: 0.7151, acc: 0.6966\n",
      "loss: 0.7163, acc: 0.6964\n",
      "loss: 0.7173, acc: 0.6976\n",
      "loss: 0.7167, acc: 0.6992\n",
      "loss: 0.7195, acc: 0.6975\n",
      "loss: 0.7183, acc: 0.6975\n",
      "> val_acc: 0.6156, val_f1: 0.5959\n",
      "                                                                                                    \n",
      "epoch: 13\n",
      "loss: 0.7264, acc: 0.7344\n",
      "loss: 0.7674, acc: 0.6589\n",
      "loss: 0.7551, acc: 0.6676\n",
      "loss: 0.7240, acc: 0.6836\n",
      "loss: 0.7186, acc: 0.6905\n",
      "loss: 0.7097, acc: 0.6983\n",
      "loss: 0.7160, acc: 0.7001\n",
      "loss: 0.7224, acc: 0.6953\n",
      "loss: 0.7218, acc: 0.6970\n",
      "loss: 0.7197, acc: 0.6990\n",
      "loss: 0.7250, acc: 0.6952\n",
      "loss: 0.7245, acc: 0.6956\n",
      "loss: 0.7221, acc: 0.6962\n",
      "loss: 0.7211, acc: 0.6960\n",
      "loss: 0.7226, acc: 0.6948\n",
      "loss: 0.7188, acc: 0.6961\n",
      "loss: 0.7206, acc: 0.6946\n",
      "loss: 0.7209, acc: 0.6939\n",
      "loss: 0.7201, acc: 0.6935\n",
      "loss: 0.7186, acc: 0.6935\n",
      "> val_acc: 0.6749, val_f1: 0.6558\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6749\n",
      "                                                                                                    \n",
      "epoch: 14\n",
      "loss: 0.6704, acc: 0.7344\n",
      "loss: 0.6668, acc: 0.7188\n",
      "loss: 0.7007, acc: 0.7067\n",
      "loss: 0.6956, acc: 0.7005\n",
      "loss: 0.6849, acc: 0.7079\n",
      "loss: 0.6821, acc: 0.7115\n",
      "loss: 0.6753, acc: 0.7135\n",
      "loss: 0.6799, acc: 0.7122\n",
      "loss: 0.6888, acc: 0.7086\n",
      "loss: 0.6841, acc: 0.7103\n",
      "loss: 0.6808, acc: 0.7111\n",
      "loss: 0.6813, acc: 0.7091\n",
      "loss: 0.6839, acc: 0.7086\n",
      "loss: 0.6849, acc: 0.7096\n",
      "loss: 0.6878, acc: 0.7087\n",
      "loss: 0.6891, acc: 0.7083\n",
      "loss: 0.6945, acc: 0.7048\n",
      "loss: 0.6919, acc: 0.7058\n",
      "loss: 0.6964, acc: 0.7046\n",
      "loss: 0.7008, acc: 0.7018\n",
      "> val_acc: 0.6777, val_f1: 0.6576\n",
      ">> saved: state_dict/lstm_twitter_val_acc0.6777\n",
      "                                                                                                    \n",
      "epoch: 15\n",
      "loss: 0.6435, acc: 0.7406\n",
      "loss: 0.6622, acc: 0.7422\n",
      "loss: 0.6819, acc: 0.7208\n",
      "loss: 0.6909, acc: 0.7148\n",
      "loss: 0.6934, acc: 0.7106\n",
      "loss: 0.6865, acc: 0.7120\n",
      "loss: 0.6823, acc: 0.7138\n",
      "loss: 0.6841, acc: 0.7133\n",
      "loss: 0.6895, acc: 0.7111\n",
      "loss: 0.6840, acc: 0.7137\n",
      "loss: 0.6815, acc: 0.7173\n",
      "loss: 0.6855, acc: 0.7141\n",
      "loss: 0.6889, acc: 0.7106\n",
      "loss: 0.6921, acc: 0.7103\n",
      "loss: 0.6906, acc: 0.7113\n",
      "loss: 0.6946, acc: 0.7068\n",
      "loss: 0.6949, acc: 0.7081\n",
      "loss: 0.6967, acc: 0.7075\n",
      "loss: 0.6969, acc: 0.7077\n",
      "> val_acc: 0.6720, val_f1: 0.6536\n",
      "                                                                                                    \n",
      "epoch: 16\n",
      "loss: 0.5701, acc: 0.8047\n",
      "loss: 0.6690, acc: 0.7321\n",
      "loss: 0.7014, acc: 0.7122\n",
      "loss: 0.6760, acc: 0.7178\n",
      "loss: 0.6738, acc: 0.7195\n",
      "loss: 0.6846, acc: 0.7159\n",
      "loss: 0.6840, acc: 0.7109\n",
      "loss: 0.6814, acc: 0.7128\n",
      "loss: 0.6815, acc: 0.7124\n",
      "loss: 0.6884, acc: 0.7078\n",
      "loss: 0.6891, acc: 0.7058\n",
      "loss: 0.6958, acc: 0.7039\n",
      "loss: 0.6919, acc: 0.7069\n",
      "loss: 0.6926, acc: 0.7057\n",
      "loss: 0.6969, acc: 0.7036\n",
      "loss: 0.6977, acc: 0.7015\n",
      "loss: 0.6969, acc: 0.7037\n",
      "loss: 0.6917, acc: 0.7062\n",
      "loss: 0.6923, acc: 0.7065\n",
      "loss: 0.6933, acc: 0.7065\n",
      "> val_acc: 0.6676, val_f1: 0.6270\n",
      "                                                                                                    \n",
      "epoch: 17\n",
      "loss: 0.7402, acc: 0.6875\n",
      "loss: 0.7136, acc: 0.6962\n",
      "loss: 0.6936, acc: 0.6987\n",
      "loss: 0.6923, acc: 0.7113\n",
      "loss: 0.6867, acc: 0.7083\n",
      "loss: 0.6850, acc: 0.7074\n",
      "loss: 0.6870, acc: 0.7105\n",
      "loss: 0.6794, acc: 0.7147\n",
      "loss: 0.6856, acc: 0.7134\n",
      "loss: 0.6837, acc: 0.7159\n",
      "loss: 0.6790, acc: 0.7182\n",
      "loss: 0.6812, acc: 0.7182\n",
      "loss: 0.6853, acc: 0.7183\n",
      "loss: 0.6857, acc: 0.7185\n",
      "loss: 0.6881, acc: 0.7145\n",
      "loss: 0.6898, acc: 0.7130\n",
      "loss: 0.6882, acc: 0.7147\n",
      "loss: 0.6907, acc: 0.7149\n",
      "loss: 0.6898, acc: 0.7149\n",
      "> val_acc: 0.6720, val_f1: 0.6547\n",
      "                                                                                                    \n",
      "epoch: 18\n",
      "loss: 0.8016, acc: 0.7188\n",
      "loss: 0.6799, acc: 0.7526\n",
      "loss: 0.6596, acc: 0.7514\n",
      "loss: 0.6552, acc: 0.7451\n",
      "loss: 0.6633, acc: 0.7366\n",
      "loss: 0.6626, acc: 0.7392\n",
      "loss: 0.6735, acc: 0.7308\n",
      "loss: 0.6762, acc: 0.7279\n",
      "loss: 0.6770, acc: 0.7245\n",
      "loss: 0.6716, acc: 0.7255\n",
      "loss: 0.6674, acc: 0.7249\n",
      "loss: 0.6752, acc: 0.7213\n",
      "loss: 0.6792, acc: 0.7198\n",
      "loss: 0.6804, acc: 0.7183\n",
      "loss: 0.6773, acc: 0.7172\n",
      "loss: 0.6729, acc: 0.7214\n",
      "loss: 0.6735, acc: 0.7191\n",
      "loss: 0.6778, acc: 0.7160\n",
      "loss: 0.6777, acc: 0.7163\n",
      "loss: 0.6820, acc: 0.7135\n",
      "> val_acc: 0.6561, val_f1: 0.6497\n",
      "                                                                                                    \n",
      "epoch: 19\n",
      "loss: 0.6411, acc: 0.7240\n",
      "loss: 0.6198, acc: 0.7598\n",
      "loss: 0.6484, acc: 0.7416\n",
      "loss: 0.6566, acc: 0.7248\n",
      "loss: 0.6606, acc: 0.7283\n",
      "loss: 0.6802, acc: 0.7165\n",
      "loss: 0.6804, acc: 0.7159\n",
      "loss: 0.6750, acc: 0.7192\n",
      "loss: 0.6736, acc: 0.7188\n",
      "loss: 0.6760, acc: 0.7184\n",
      "loss: 0.6714, acc: 0.7220\n",
      "loss: 0.6739, acc: 0.7185\n",
      "loss: 0.6754, acc: 0.7173\n",
      "loss: 0.6750, acc: 0.7185\n",
      "loss: 0.6788, acc: 0.7166\n",
      "loss: 0.6778, acc: 0.7188\n",
      "loss: 0.6778, acc: 0.7180\n",
      "loss: 0.6806, acc: 0.7157\n",
      "loss: 0.6826, acc: 0.7144\n",
      "loss: 0.6795, acc: 0.7165\n",
      "> val_acc: 0.6734, val_f1: 0.6488\n",
      ">> test_acc: 0.6777, test_f1: 0.6576\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ram'\n",
    "dataset = 'twitter' # twitter, laptop， restaurant\n",
    "optimizer = 'adam'\n",
    "initializer = 'xavier_uniform_'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "log_file = '{}-{}-{}.log'.format(model_name, dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "opt_lstm = Parameter(model_classes[model_name], dataset_files[dataset], input_colses[model_name], \n",
    "              initializers[initializer], optimizers[optimizer], model_name, dataset, learning_rate, 20)\n",
    "\n",
    "ins = Instructor(opt_lstm)\n",
    "ins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
